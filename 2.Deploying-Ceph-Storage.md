```text
作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. 邮箱：939958092@qq.com
```

# 部署红帽 Ceph 存储

## 准备集群部署

cephadm 实用程序由两个主要组件组成：

- `cephadm`

- `cephadm orchestrator`

### 规划服务托管

所有集群服务现在都作为容器运行。容器化 Ceph 服务可在同一节点运行；这被称为`colocation`。<mark>并置 Ceph 服务可以提高资源利用率</mark>，并保持服务之间的安全隔离。

以下守护进程可以和 OSD 守护进程并置：RADOSGW、MDS、RBD-mirror、MON、MGR、Grafana 和 NFS Ganesha。

### 节点之间的安全通讯

cephadm命令使用SSH协议与存储集群节点通信。集群SSH Key是在集群引导过程中创建的。<mark>应该将集群公钥复制到每个主机</mark>，集群公钥位于： `/etc/ceph`

## 部署新集群

部署新集群的步骤如下：

- 在引导节点（集群中的第一个节点）主机上安装 `cephadm-ansible` 软件包。

- 运行 cephadm 预检 playbook。此 playbook 将验证主机是否具备必要条件。

- 使用 cephadm 引导集群。引导进程将完成以下任务：
  
  - 在引导节点上安装并启动 Ceph 监控器和 Ceph 管理器守护进程。
  
  - 创建 `/﻿etc/﻿ceph` 目录。
  
  - 将集群 SSH 公钥的副本写入 `/etc/ceph/ceph.pub`，并将密钥添加到 `/root/.ssh/authorized_keys` 文件。
  
  - 将与新集群通信所需的最小配置文件写入 `/etc/ceph/ceph.conf` 文件。
  
  - 将 `client.admin` 管理机密密钥的副本写入 `/etc/ceph/ceph.client.admin.keyring` 文件。
  
  - 使用 `prometheus` 和 `grafana` 服务以及 `node-exporter` 和 `alert-manager` 等其他工具部署基础监控堆栈。

### 安装先决条件

在引导节点上安装cephadm-ansible

```bash
[root@serverc ~]# yum install cephadm-ansible -y
```

运行 `cephadm-preflight.yaml` playbook。此 playbook 将配置 Ceph 存储库，并为引导准备存储集群。除此之外，还会安装 `podman`、`lvm2`、`chrony`、`cephadm` 等必备软件包。

inventory默认位置是/usr/share/cephadm-ansible/hosts，下面的例子展示了一个典型的inventory文件的结构:

```ini
[root@serverc ~]# cd /usr/share/cephadm-ansible/
[root@serverc cephadm-ansible]# cat hosts
clienta.lab.example.com
serverc.lab.example.com
serverd.lab.example.com
servere.lab.example.com
```

运行cephadm-preflight

对于⽣产环境，将 ceph_origin 设置为rhcs，以便为部署启⽤红帽存储⼯具存储库

```bash
[root@serverc cephadm-ansible]# ansible-playbook -i hosts cephadm-preflight.yml --extra-vars "ceph_origin="
```

### 引导集群

cephadm 引导进程在单个节点上创建由一个 Ceph 监控器、一个 Ceph 管理器和必要依赖项组成的小型存储集群

#### 使用cephadm bootstrap引导新集群

```bash
[root@serverc ~]# cephadm bootstrap --mon-ip=MON_IP \
--registry-url=registry.redhat.io \
--registry-username=REGISTRY_USERNAME \
--registry-password=REGISTRY_PASSWORD \ 
--initial-dashboard-password=DASHBOARO_PASSWORO \
--dashboard-password-noupdate \ 
--allow-fqdn-hostname
```

#### 使用服务规范文件

使用带有 `--apply-spec` 选项的 `cephadm bootstrap` 命令和服务规范文件来引导存储集群并配置额外的主机和守护进程。配置文件是一个 YAML 文件，其中包含待部署服务的服务类型、放置和指定节点。

以下是服务配置文件示例：

```bash
vim cluster.yaml
```

```yaml
service_type: host
addr: 172.25.250.10
hostname: clienta.lab.example.com
---
service_type: host
addr: 172.25.250.12
hostname: serverc.lab.example.com
---
service_type: host
addr: 172.25.250.13
hostname: serverd.lab.example.com
---
service_type: host
addr: 172.25.250.14
hostname: servere.lab.example.com
---
service_type: mon
placement:
  hosts:
    - clienta.lab.example.com
    - serverc.lab.example.com
    - serverd.lab.example.com
    - servere.lab.example.com
---
service_type: rgw
service_id: realm.zone
placement:
  hosts:
    - serverc.lab.example.com
    - serverd.lab.example.com
---
service_type: mgr
placement:
  hosts:
    - clienta.lab.example.com
    - serverc.lab.example.com
    - serverd.lab.example.com
    - servere.lab.example.com
---
service_type: osd
service_id: default_drive_group
placement:
  host_pattern: 'server*'
data_devices:
  paths:
    - /dev/vdb
    - /dev/vdc
    - /dev/vdd
```

```bash
cephadm bootstrap --mon-ip=172.25.250.12 \
--apply-spec=cluster.yaml \
--initial-dashboard-password=redhat \
--dashboard-password-noupdate \
--allow-fqdn-hostname \
--registry-url=registry.lab.example.com \
--registry-username=registry \
--registry-password=redhat
```

运行结束后，会输出以下内容

```bash
Ceph Dashboard is now available at:

             URL: https://serverc.lab.example.com:8443/
            User: admin
        Password: redhat
```

## 为集群节点打标签

添加标签有助于识别每个主机上运行的守护进程，从而简化集群管理任务。

除 `_admin` 标签外，标签都是自由格式，没有特殊含义

例如，下面的命令将_ admin标签应用到clienta主机，以指定为admin节点

```bash
[root@serverc ~]# ceph orch host ls
HOST                     ADDR           LABELS  STATUS
clienta.lab.example.com  172.25.250.10
serverc.lab.example.com  172.25.250.12
serverd.lab.example.com  172.25.250.13
servere.lab.example.com  172.25.250.14
```

```bash
[root@serverc ~]# ceph orch host label add clienta.lab.example.com _admin
Added label _admin to host clienta.lab.example.com
[root@serverc ~]# ceph orch host ls
HOST                     ADDR           LABELS  STATUS
clienta.lab.example.com  172.25.250.10  _admin
serverc.lab.example.com  172.25.250.12
serverd.lab.example.com  172.25.250.13
servere.lab.example.com  172.25.250.14
```

使用标签将集群守护进程部署到特定的主机

```bash
[root@serverc ~]# ceph orch apply prometheus --placement="label:prometheus"
```

## 设置Admin节点

配置admin节点的步骤如下:

1. 将admin标签分配给节点

2. 复制admin密钥到管理节点

3. 复制ceph.conf文件到admin节点

```bash
[root@serverc ~]# cd /etc/ceph
[root@serverc ceph]# scp {ceph.client.admin.keyring,ceph.conf} \
> root@clienta:/etc/ceph/
Warning: Permanently added 'clienta' (ECDSA) to the list of known hosts.
ceph.client.admin.keyring                                              100%   63    43.4KB/s   00:00
ceph.conf                                                              100%  177   197.3KB/s   00:00
```

# 扩展红帽 Ceph 存储集群容量

可以通过两种方式扩展集群中的存储：

- 添加额外 OSD 节点到集群，又称*横向扩展*。

- 添加额外存储空间到现有 OSD 节点，又称*纵向扩展*。

使用 `cephadm shell -- ceph health` 命令确认集群是否已处于 `HEALTH_OK` 状态，然后再开始部署额外的 OSD。

## 配置更多的OSD服务器

作为存储管理员，可以向Ceph存储集群添加更多主机，以维护集群健康并提供足够的负载容量。在当前存储空间已满的情况下，可以通过增加一个或多个osd来增加集群存储容量。

### 分发ssh密钥

以 root 用户身份，将 Ceph 存储集群 SSH 公钥添加到新主机上 root 用户的 `authorized_keys` 文件。

```bash
[root@clienta ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@new-osd-1
```

### 检查并配置先决条件

更新inventory后，使用--limit选项运行preflight剧本，以限制剧本的任务只在指定的节点上运行

```bash
[root@clienta ~]# ansible-playbook -i /usr/share/cephadm-ansible/hosts/ \
/usr/share/cephadm-ansible/cephadm-preflight.yml \
--limit new-osd-1
```

### 选择添加主机的方法

#### 使用命令

在 Cephadm shell 中，以 root 用户身份使用 `ceph orch host add` 命令将新主机添加到存储集群

```bash
[ceph: root@clienta /]# ceph orch host add new-osd-1 --labels=mon,osd,mgr
```

#### 使用规范文件添加多个主机

要添加多个主机，创建一个包含主机描述的YAML文件，在管理容器中创建YAML文件，然后运行ceph orch

```yaml
service_type: host
addr:
hostname: new-osd-1
labels:
  - mon
  - osd
  - mgr
---
service_type: host
addr:
hostname: new-osd-2
labels:
  - mon
  - osd
```

使用ceph orch apply添加OSD服务器

```bash
[ceph: root@adm ~]# ceph orch apply -i host.yaml
Added host 'new-osd-1' with addr '192.168.122.102'
Added host 'new-osd-1' with addr '192.168.122.103'
```

## 列出主机

从 cephadm shell 中使用 `ceph orch host ls` 列出集群节点。当主机在线并正常运行时，`STATUS` 列为空。

```bash
[ceph: root@adm /]# ceph orch host ls
HOST            ADDR             LABELS       STATUS
existing-osd-1  192.168.122.101  mon
new-osd-1       192.168.122.102  mon osd mgr
new-osd-2       192.168.122.103  mon osd
```

## 为OSD服务器配置额外的OSD存储

Ceph 要求满足以下条件才能考虑使用存储设备：

- 设备不得有分区。

- 设备不得处于 LVM 状态。

- 设备不得挂载。

- 设备不得包含文件系统。

- 设备不得包含 Ceph BlueStore OSD。

- 设备必须大于 5 GB。

从 cephadm shell 运行 `ceph orch device ls` 命令，以列出可用设备。`--wide` 选项提供更多设备详细信息

```bash
[root@serverc ~]# ceph orch device ls
Hostname                 Path      Type  Serial  Size   Health   Ident  Fault  Available
clienta.lab.example.com  /dev/vdb  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vdc  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vdd  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vde  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vdf  hdd           10.7G  Unknown  N/A    N/A    Yes
serverc.lab.example.com  /dev/vde  hdd           10.7G  Unknown  N/A    N/A    Yes
serverc.lab.example.com  /dev/vdf  hdd           10.7G  Unknown  N/A    N/A    Yes
serverc.lab.example.com  /dev/vdb  hdd           10.7G  Unknown  N/A    N/A    No
serverc.lab.example.com  /dev/vdc  hdd           10.7G  Unknown  N/A    N/A    No
serverc.lab.example.com  /dev/vdd  hdd           10.7G  Unknown  N/A    N/A    No
serverd.lab.example.com  /dev/vde  hdd           10.7G  Unknown  N/A    N/A    Yes
serverd.lab.example.com  /dev/vdf  hdd           10.7G  Unknown  N/A    N/A    Yes
serverd.lab.example.com  /dev/vdb  hdd           10.7G  Unknown  N/A    N/A    No
serverd.lab.example.com  /dev/vdc  hdd           10.7G  Unknown  N/A    N/A    No
serverd.lab.example.com  /dev/vdd  hdd           10.7G  Unknown  N/A    N/A    No
servere.lab.example.com  /dev/vde  hdd           10.7G  Unknown  N/A    N/A    Yes
servere.lab.example.com  /dev/vdf  hdd           10.7G  Unknown  N/A    N/A    Yes
servere.lab.example.com  /dev/vdb  hdd           10.7G  Unknown  N/A    N/A    No
servere.lab.example.com  /dev/vdc  hdd           10.7G  Unknown  N/A    N/A    No
servere.lab.example.com  /dev/vdd  hdd           10.7G  Unknown  N/A    N/A    No

```

以 root 用户身份运行 `ceph orch daemon add osd` 命令，以便使用特定主机上的特定设备创建 OSD

可以看到，添加了osd服务后，vdf的状态就从yes变成了no

```bash
[root@serverc ~]# ceph orch daemon add osd servere.lab.example.com:/dev/vdf
Created osd(s) 9 on host 'servere.lab.example.com'
[root@serverc ~]# ceph orch device ls | grep servere | grep vdf
servere.lab.example.com  /dev/vdf  hdd           10.7G  Unknown  N/A    N/A    No
```

或者，运行 `ceph orch apply osd --all-available-devices` 命令，以便在所有可用和未使用的设备上部署 OSD

```bash
[root@serverc ~]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
```

可以仅使用特定主机上的特定设备创建osd，下以下示例会在各主机上由 `/dev/vdc` 和 `/dev/vdd` 支持的组 `default_drive_group` 中创建两个 OSD

```bash
vim osd_spec.yml
```

```yaml
service_type: osd
service_id: default_drive_group
placement:
  hosts:
    - serverc.lab.example.com
    - serverd.lab.example.com
    - servere.lab.example.com
data_devices:
  paths:
    - /dev/vdb
    - /dev/vdc
    - /dev/vdd
```

运行 `ceph orch apply` 命令来实现 YAML 文件中的配置。

```bash
[root@serverc ~]# ceph orch apply -i osd_spec.yml
Scheduled osd.default_drive_group update...
```

## 查询osd信息

使用 `ceph osd tree` 命令显示 CRUSH 树。验证新 OSD 在基础架构中的位置正确无误

```bash
[root@serverc ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.09796  root default
-3         0.02939      host serverc
 0    hdd  0.00980          osd.0         up   1.00000  1.00000
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
 2    hdd  0.00980          osd.2         up   1.00000  1.00000
-5         0.02939      host serverd
 3    hdd  0.00980          osd.3         up   1.00000  1.00000
 5    hdd  0.00980          osd.5         up   1.00000  1.00000
 7    hdd  0.00980          osd.7         up   1.00000  1.00000
-7         0.03918      host servere
 4    hdd  0.00980          osd.4         up   1.00000  1.00000
 6    hdd  0.00980          osd.6         up   1.00000  1.00000
 8    hdd  0.00980          osd.8         up   1.00000  1.00000
 9    hdd  0.00980          osd.9         up   1.00000  1.00000

```

使用 `ceph osd df` 命令，验证数据使用量，以及每个 OSD 的放置组数量

```bash
[root@serverc ~]# ceph osd df
ID  CLASS  WEIGHT   REWEIGHT  SIZE     RAW USE  DATA     OMAP     META     AVAIL    %USE  VAR   PGS  STATUS
 0    hdd  0.00980   1.00000   10 GiB   14 MiB  2.4 MiB      0 B   12 MiB   10 GiB  0.14  0.71   34      up
 1    hdd  0.00980   1.00000   10 GiB   24 MiB  2.4 MiB      0 B   21 MiB   10 GiB  0.23  1.18   39      up
 2    hdd  0.00980   1.00000   10 GiB   24 MiB  2.4 MiB      0 B   22 MiB   10 GiB  0.24  1.20   32      up
 3    hdd  0.00980   1.00000   10 GiB   23 MiB  2.4 MiB      0 B   20 MiB   10 GiB  0.22  1.14   35      up
 5    hdd  0.00980   1.00000   10 GiB   26 MiB  2.4 MiB      0 B   24 MiB   10 GiB  0.25  1.30   34      up
 7    hdd  0.00980   1.00000   10 GiB   15 MiB  2.4 MiB      0 B   13 MiB   10 GiB  0.15  0.76   36      up
 4    hdd  0.00980   1.00000   10 GiB   23 MiB  2.4 MiB      0 B   20 MiB   10 GiB  0.22  1.13   25      up
 6    hdd  0.00980   1.00000   10 GiB   15 MiB  2.4 MiB      0 B   13 MiB   10 GiB  0.15  0.76   25      up
 8    hdd  0.00980   1.00000   10 GiB   22 MiB  2.4 MiB      0 B   20 MiB   10 GiB  0.22  1.10   25      up
 9    hdd  0.00980   1.00000   10 GiB   15 MiB  2.4 MiB      0 B   12 MiB   10 GiB  0.14  0.72   30      up
                       TOTAL  100 GiB  201 MiB   24 MiB  8.5 KiB  177 MiB  100 GiB  0.20
MIN/MAX VAR: 0.71/1.30  STDDEV: 0.04

```


