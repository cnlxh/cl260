```text
作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. 邮箱：939958092@qq.com
```

# 使用逻辑卷创建 BlueStore OSD

## BlueStore 简介

BlueStore 取代 FileStore 作为 OSD 的存储后端，BlueStore 将对象直接存储在原始块设备上，<mark>免除了对文件系统层的需要，从而提高了读写操作速度</mark>

## BlueStore 架构

Ceph 集群中存储的对象在整个集群具有<mark>唯一标识符、二进制对象数据和对象元数据</mark>。BlueStore 将对象元数据存储在*块数据库*中。块数据库将元数据作为键值对存储在 *RocksDB* 数据库中，这是一种高性能的键值存储。

块数据库驻留于存储设备上的一个小型 BlueFS 分区。BlueFS 是最小的文件系统，用于保存 RocksDB 文件。

BlueStore 利用*预写式日志* (*WAL*) 将数据写入块存储设备。预写式日志执行日志记录功能，并记录所有事务。

![](https://gitee.com/cnlxh/cl260/raw/master/images/component/component-bluestore-ceph-osd.svg)

## BlueStore 性能

FileStore 写入到日志，然后从日志中写入到块设备。BlueStore 可避免这种双重写入的性能损失，<mark>直接将数据写入块设备</mark>，同时<mark>使用单独的数据流将事务记录到预写式日志</mark>。当工作负载相似时，<mark>BlueStore 的写操作速度约为 FileStore 的两倍</mark>。

在混用不同的集群存储设备时，自定义 BlueStore OSD 可以提高性能。创建 BlueStore OSD 时，默认会将数据、块数据库和预写式日志放置到同一个块设备上。块数据库和预写式日志有很多性能优势，所以将这些组件放在单独的、速度更快的设备上也许可以提高性能。

```bash
[root@serverc ~]# cat bluestore.yml
service_type: osd
service_id: lxh-bluestore-test
placement:
  host_pattern: 'serverc*'
data_devices:
  paths:
    - /dev/vde
wal_devices:
  paths:
    - /dev/vdf

[root@serverc ~]# ceph orch apply -i bluestore.yml
Scheduled osd.lxh-bluestore-test update...

[root@serverc ~]# lsblk
NAME                        MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
vde                         252:64   0   10G  0 disk
└─ceph--aba9b-'block'       253:3    0   10G  0 lvm
vdf                         252:80   0   10G  0 disk
└─ceph--110ee-'wal'         253:4    0   10G  0 lvm
```

```bash
[root@serverc ~]# ceph orch ls --service-type osd --format yaml
---
service_type: osd
service_id: lxh-bluestore-test
service_name: osd.lxh-bluestore-test
placement:
  host_pattern: serverc*
spec:
  data_devices:
    paths:
    - /dev/vde
  filter_logic: AND
  objectstore: bluestore
  wal_devices:
    paths:
    - /dev/vdf
status:
  created: '2023-09-08T18:40:58.116841Z'
  running: 0
  size: 1
```

BlueStore 存储后端提供以下功能：

- 允许为数据、块数据库和预写式日志 (WAL) 使用单独的设备。

- 几乎支持使用 HDD、SSD 和 NVMe 设备的任意组合。

- 在原始设备或分区上操作，从而消除对存储设备的双重写入，同时提高元数据效率。

- 使用校验和写入所有数据和元数据。在返回至客户端之前，所有读取操作都使用相对应的校验和进行验证。

## 添加OSD到集群

先查看现有服务器上的硬盘情况

```bash
[root@serverc ~]# ceph orch device ls
Hostname                 Path      Type  Serial  Size   Health   Ident  Fault  Available
clienta.lab.example.com  /dev/vdb  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vdc  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vdd  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vde  hdd           10.7G  Unknown  N/A    N/A    Yes
clienta.lab.example.com  /dev/vdf  hdd           10.7G  Unknown  N/A    N/A    Yes
```

将clienta上的/dev/vdb盘加进来

```bash
[root@serverc ~]# ceph orch daemon add osd clienta.lab.example.com:/dev/vdb
Created osd(s) 11 on host 'clienta.lab.example.com'
```

再次查看osd情况，发现在clienta上多了一个osd

```bash
[root@serverc ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-1         0.11755  root default
-9         0.00980      host clienta
11    hdd  0.00980          osd.11        up   1.00000  1.00000
-3         0.03918      host serverc
 0    hdd  0.00980          osd.0         up   1.00000  1.00000
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
 2    hdd  0.00980          osd.2         up   1.00000  1.00000
10    hdd  0.00980          osd.10        up   1.00000  1.00000
-5         0.02939      host serverd
 3    hdd  0.00980          osd.3         up   1.00000  1.00000
 5    hdd  0.00980          osd.5         up   1.00000  1.00000
 7    hdd  0.00980          osd.7         up   1.00000  1.00000
-7         0.03918      host servere
 4    hdd  0.00980          osd.4         up   1.00000  1.00000
 6    hdd  0.00980          osd.6         up   1.00000  1.00000
 8    hdd  0.00980          osd.8         up   1.00000  1.00000
 9    hdd  0.00980          osd.9         up   1.00000  1.00000
```

## 擦除硬盘准备OSD

如果硬盘并不是空的，可以用下面的命令让设备做好调配准备，<mark>此命令将删除所有分区并清除设备中的数据</mark>

<mark>需要注意的是，这个命令并不能擦除osd，只能擦除非ceph内的外部硬盘</mark>

```bash
[root@serverc ~]# ceph orch device zap clienta.lab.example.com /dev/vdc --force
```

## 自动创建OSD

编排器服务可在集群主机中发现可用设备，添加设备并创建 OSD 守护进程

使用 `ceph orch apply osd --all-available-devices` 命令调配所有可用而未使用的设备。

```bash
[root@serverc ~]# ceph orch apply osd --all-available-devices
Scheduled osd.all-available-devices update...
```

## 停止自动应用osd

如果前面我们已经添加了所有硬盘到集群中，而且添加了--all-available-devices选项，将会导致我们删除osd后再次重新创建，我们先停止集群自动创建osd

```bash
[root@serverc ~]# ceph orch apply osd --all-available-devices --unmanaged=true
Scheduled osd.all-available-devices update...
```

## 从集群中删除OSD

这里有一个很重要的问题，一般情况下我们不会主动删除某个osd，通常是它坏了，主动报告状态为down，这个问题是我们如何知道哪个osd是哪块盘，例如我明确知道host2上的/dev/nvme0n3坏了，那这是哪个osd呢，或者反过来，我就想删除某个osd，我如何知道它是哪块盘呢

从上面的信息我们看到host2上的osd分别为2 4 7 10，一共4个osd，现在我想删除osd 10，并把盘用来干别的

### 查询osd所在的硬盘名称

查询osd.1在哪个服务器上的哪块硬盘

经过查询，发现osd.0属于`"serverc.lab.example.c` 的vdb

```bash
[root@serverc ~]# ceph osd metadata 0 | grep -E 'devices|hostname'
    "bluestore_bdev_devices": "vdb",
    "container_hostname": "serverc.lab.example.com",
    "devices": "vdb",
    "hostname": "serverc.lab.example.com",
    "objectstore_numa_unknown_devices": "vdb",
```

### OSD暂停服务

暂停服务后，权重降低为0

```bash
[root@serverc ~]# ceph osd out 0
marked out osd.0.

[root@serverc ~]# ceph osd tree
ID  CLASS  WEIGHT   TYPE NAME         STATUS  REWEIGHT  PRI-AFF
-3         0.03918      host serverc
 0    hdd  0.00980          osd.0         up         0  1.00000
 1    hdd  0.00980          osd.1         up   1.00000  1.00000
```

### 停止OSD

stop之后，相应的服务也会停止

```bash
[root@serverc ~]# ceph orch daemon stop osd.0
Scheduled to stop osd.0 on host 'serverc.lab.example.com'

[root@serverc ~]# systemctl is-active ceph-2ae6d05a-229a-11ec-925e-52540000fa0c@osd.0.service
inactive
```

### 删除OSD

```bash
[root@serverc ~]# ceph orch osd rm 0
Scheduled OSD(s) for removal
```

如果在rm之后，发现crush并没有删除，可以手工删除

```bash
[root@serverc ~]# ceph osd crush rm osd.0
```

### 确认OSD Secret已删除

```bash
[root@serverc ~]# ceph auth list | grep osd.0
installed auth entries:

osd.0
```

如果还有残留，会导致下次创建osd失败，通过以下方式删除

```bash
[root@serverc ~]# ceph auth del osd.0
```

# 创建和配置池

## 了解池的含义

池是存储对象的逻辑分区，Ceph客户端将对象写入池，池为集群提供了一层弹性，因为<mark>池定义了可以出现故障而不会丢失数据的 OSD 数量。</mark>

## 池类型

可用的池类型有`Replicated pools`和`Erasure pools`

默认池类型为 `replicated`，通过将各个对象复制到多个 OSD 来发挥作用。此池类型<mark>需要较多存储空间，但提供更好的性能</mark>

<mark>纠删代码池需要较少的存储空间和网络带宽</mark>，但因为要进行奇偶校验计算，所以<mark>会占用较多的 CPU 处理时间</mark>

- 对于<mark>不需要频繁访问</mark>且<mark>不需要低延迟</mark>的数据，推荐使用<mark>纠删代码池</mark>。

- 对于<mark>需要频繁访问</mark>并且要具备<mark>快速读取</mark>性能的数据，推荐使用<mark>复制池</mark>

一旦完成池创建，<mark>池类型便无法更改</mark>。

## 创建复制池

Ceph 通过<mark>创建每个对象的多个副本</mark>来保护复制池中的数据，这称为副本 (replica)。Ceph 使用 <mark>CRUSH 故障域来确定存储数据</mark>的操作集的主要 OSD。然后，主要 OSD 会查找池的当前副本大小，并计算要写入对象的次要 OSD。在主要 OSD 收到写入确认并完成数据写入后，主要 OSD 会向 Ceph 客户端确认写入操作已成功。如果一个或多个 OSD 出现故障，这一过程可保护对象中的数据。

使用以下命令来创建复制池。

```bash
[root@serverc ~]# ceph osd pool create pool-name  pg-num  pgp-num  replicated crush-rule-name
```

其中：

- `pool_name` 是新池的名称。

- `pg_num` 是为这个池配置的放置组 (PG) 总数。

- `pgp_num` 是这个池的有效放置组数量。将它设置为与 `pg_num` 相等。

- `replicated` 指定这是复制池；如果命令中未包含此参数，这是默认值。

- `crush-rule-name` 是您想要用于这个池的 CRUSH 规则集的名称。`osd_pool_default_crush_replicated_ruleset` 配置参数设置其默认值。

创建一个名为`lxhpool`，且pg和pgp都是32的复制池，<mark>默认就是复制池，所以最后的replicated并不需要指定，而且在Ceph 5之后的集群中，也不需要指定PG参数，会自动平衡</mark>

```bash
[root@serverc ~]# ceph osd pool create lxhpool 32 32 replicated
pool 'lxhpool' created
```

查看数据库中定义的副本数是3，`osd_pool_default_min_size` 参数可设置必须提供多少个对象副本才能接受 I/O 请求。默认值为 2。

```bash
[root@serverc ~]# ceph config get mon osd_pool_default_size
3
```

## 查看池详细属性

可以看到lxhpool的副本数量为3，也就3副本存储

```bash
[root@serverc ~]# ceph osd pool ls detail | grep lxhpool
pool 6 'lxhpool' replicated size 3 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on 
```

## 修改池的副本数

```bash
[root@serverc ~]# ceph osd pool set lxhpool size 4
set pool 6 size to 4

[root@serverc ~]# ceph osd pool ls detail | grep lxhpool
pool 6 'lxhpool' replicated size 4 min_size 2 crush_rule 0 object_hash rjenkins pg_num 32 pgp_num 32 autoscale_mode on
```

## 配置池的应用程序类型

Ceph 池的应用程序类型主要有以下几种：

1. **RBD（Ceph Block Device）**：用于块设备存储，类似于传统的SAN或RAID设备，支持高性能随机读写的需求，通常用于数据库、虚拟机等需要快速磁盘访问的应用场景

2. **CephFS**：提供分布式文件系统服务，支持POSIX语义，并提供高性能的文件读写操作，适用于需要文件系统服务的场景

3. **RGW（RADOS Gateway）**：提供基于对象的存储服务，类似于Amazon S3或OpenStack Swift等云存储服务，支持RESTful API的访问方式，适用于存储大量非结构化数据，如图片、视频、日志等

将lxhpool类型设置为rbd

```bash
[root@serverc ~]# ceph osd pool application enable lxhpool rbd
enabled application 'rbd' on pool 'lxhpool'
[root@serverc ~]# ceph osd pool application get lxhpool
{
    "rbd": {}
}
```

## 修改池名称

lxhpool没有体现出IT用途，我们修改一下

```bash
[root@serverc ~]# ceph osd pool rename lxhpool ftpserver
pool 'lxhpool' renamed to 'ftpserver'
[root@serverc ~]# ceph osd pool ls
...
ftpserver
```

## 是否允许删除池

一般来说，如果全局允许删除池，你可以用下面的方法来修改池的删除开关

设置为true，就是不允许删除

```bash
[root@serverc ~]# ceph config set mon mon_allow_pool_delete true
[root@serverc ~]# ceph osd pool set ftpserver nodelete true
set pool 6 nodelete to true
[root@serverc ~]# ceph osd pool rm ftpserver ftpserver --yes-i-really-really-mean-it
Error EPERM: pool deletion is disabled; you must unset nodelete flag for the pool first
```

## 纠删代码池概述

在纠删代码池中存储的对象分割为多个数据区块，这些数据区块存储在不同的 OSD 中。<mark>编码块的数量是根据数据块计算的</mark>，并存储在不同的 OSD 中。<mark>当 OSD 出现故障时，可利用编码块来重构对象数据</mark>

与复制池不同，<mark>纠删代码池不依赖于存储每个对象的多个副本</mark>。

以下概括了纠删代码池的工作方式：

- 每个对象的数据分割为 `k` 个数据区块。

- 计算出 `m` 个编码区块。

- 编码区块大小与数据区块大小相同。

- 对象存储在总共 `k + m` 个 OSD 中。

![](https://gitee.com/cnlxh/cl260/raw/master/images/component/configuring-config-erasure.svg)

与复制池相比，<mark>纠删代码使用存储容量的效率更高</mark>。复制池维护对象的 n 个副本，而<mark>纠删代码仅维护 k + m 个区块</mark>。例如，具有 3 个副本的复制池要使用 3 倍存储空间。而 k=4 和 m=2 的纠删代码池仅要使用 1.5 倍存储空间。

**假设我们有一个原始数据文件，大小为1GB**

```text
复制池：1*3=3G

纠删码池：k=4, m=2

每个数据块=1G/4=250M

总的编码块=250M*2=500M

总大小等于=250M*4个数据块+编码块500M=1500M
```

<mark>综上所述，在Ceph分布式存储中，如果用复制池，需要3G空间，而纠删码池只需要1.5G，所以更省空间</mark>

与复制池相比，<mark>纠删代码池</mark>只需<mark>较少的存储空间</mark>就能<mark>实现相似程度的数据保护</mark>，由此可以<mark>降低成本和存储集群的大小</mark>。不过，计算编码块会增加纠删代码池的<mark> CPU 处理和内存开销</mark>，因而<mark>会降低整体性能</mark>。

## 创建纠删代码池

命令格式如下：

```bash
ceph osd pool create pool-name pg-num pgp-num erasure erasure-code-profile crush-rule-name
```

其中：

- `pool-name` 是新池的名称。

- `pg-num` 是这个池的放置组 (PG) 总数。

- `pgp-num` 是这个池的有效放置组数量。通常而言，这应当与 PG 总数相等。

- `erasure` 指定这是纠删代码池。

- `erasure-code-profile` 要使用的配置文件的名称。您可使用 `ceph osd erasure-code-profile set` 命令创建新的配置文件。配置文件定义 `k` 和 `m` 值，以及要使用的纠删代码池插件。默认情况下，Ceph 使用 `default` 配置文件。

- `crush-rule-name` 是要用于这个池的 CRUSH 规则集的名称。如果不设置，Ceph 将使用纠删代码池配置文件中定义的规则集。

使用以下命令创建Erasure编码池

```bash
[root@serverc ~]# ceph osd pool create ecpool erasure default
pool 'ecpool' created
[root@serverc ~]# ceph osd pool ls detail | grep ecpool
pool 7 'ecpool' erasure profile default size 4 min_size 3 crush_rule 1 
```

## 纠删代码配置文件概述

纠删代码配置文件可配置纠删代码池用于存储对象的数据区块和编码区块的数量，以及要使用的纠删代码插件和算法。

创建配置文件来定义一组不同的纠删代码参数。Ceph 会在安装期间创建 `default` 配置文件。这个配置文件已配置为将对象分割为两个数据区块和一个编码区块。

使用以下命令来创建新配置文件。

```bash
ceph osd erasure-code-profile set profile-name arguments
```

可用的参数如下：

`k`

在不同 OSD 之间拆分的数据区块数量。默认值为 2。

`m`

数据变得不可用之前可以出现故障的 OSD 数量。默认值为 1。

`directory`

此可选参数是插件库的位置。默认值为 `/usr/lib64/ceph/erasure-code`。

`plugin`

此可选参数定义要使用的纠删代码算法。

`crush-failure-domain`

此可选参数定义 CRUSH 故障域，它控制区块放置。默认设置为 `host`，这样可确保对象的区块放置到不同主机的 OSD 上。如果设置为 `osd`，则对象的区块可以放置到同一主机的 OSD 上。如果将故障域设置为 `osd`，则会缺乏一定的灵活性，因为如果主机出现故障，则该主机上的所有 OSD 都会出现故障。故障域可定义，并可用于确保将区块放置到不同数据中心机架或其他定制的主机上的 OSD 上。

`crush-device-class`

此可选参数选择仅将这一类别设备支持的 OSD 用于池。典型的类别可能包括 `hdd`、`ssd` 或 `nvme`。

`crush-root`

此可选参数设置 CRUSH 规则集的根节点。

``key=value``

插件可以具有对该插件唯一的键值参数。

`technique`

每个插件提供一组不同的技术来实施不同的算法。

不能修改已存在存储池的erasure code配置文件

使用 `ceph osd erasure-code-profile ls` 命令可列出现有的配置文件。

使用 `ceph osd erasure-code-profile get` 命令可查看现有配置文件的详细信息。

使用 `ceph osd erasure-code-profile rm` 命令可移除现有的配置文件。

## 管理和应用纠删配置文件

列出所有配置文件

```bash
[root@serverc ~]# ceph osd erasure-code-profile ls
default
```

查看default配置文件详情

```bash
[root@serverc ~]# ceph osd erasure-code-profile get default
k=2
m=2
plugin=jerasure
technique=reed_sol_van
```

使用以下命令创建一个新的配置文件

```bash
[root@serverc ~]# ceph osd erasure-code-profile set lxhecprofile \
crush-device-class=hdd \
crush-failure-domain=host \
crush-root=default \
k=4 m=2

[root@serverc ~]# ceph osd erasure-code-profile get lxhecprofile
crush-device-class=hdd
crush-failure-domain=host
crush-root=default
jerasure-per-chunk-alignment=false
k=4
m=2
plugin=jerasure
technique=reed_sol_van
w=8
```

<mark>修改池现有的配置文件是不支持的，必须要创建新的池</mark>

```bash
[root@serverc ~]# ceph osd pool create ecpool2 erasure lxhecprofile
pool 'ecpool2' created
[root@serverc ~]# ceph osd pool ls detail | grep lxhecprofile
pool 8 'ecpool2' erasure profile lxhecprofile size 6 min_size 5
```

## 池命名空间

*命名空间*是池中对象的逻辑组。可以限制池的访问权限，使得用户只能存储或检索特定命名空间内的对象。命名空间的一个优点是可以将用户访问权限限制到池的某一部分。

若要在命名空间内存储对象，客户端应用必须提供池和命名空间的名称。默认情况下，每个池包含一个具有空名称的命名空间，称为默认命名空间。

```bash
[root@serverc ~]# rados -p ecpool put hosts /etc/hosts
[root@serverc ~]# rados -p ecpool -N lixiaohui put hosts /etc/hosts
[root@serverc ~]# rados -p ecpool -N lixiaohui put inittab /etc/inittab
[root@serverc ~]# rados -p ecpool ls --all
lixiaohui       inittab
hosts
lixiaohui       hosts
```

```bash
[root@serverc ~]# rados -p ecpool ls --all --format=json-pretty
[
    {
        "namespace": "lixiaohui",
        "name": "inittab"
    },
    {
        "namespace": "",
        "name": "hosts"
    },
    {
        "namespace": "lixiaohui",
        "name": "hosts"
    }
]
```

# 管理 Ceph 身份验证

## 用户身份验证

红帽 Ceph 存储会<mark>使用 `cephx` 协议来授权集群</mark>中客户端、应用和守护进程之间的通信。`cephx` 协议基于<mark>共享机密密钥</mark>运行。

Ceph 会使用用户帐户实现多种用途：

- Ceph 守护进程之间的内部通信。

- 通过 `librados` 库访问集群的客户端应用。

- 集群管理员。

Ceph 守护进程使用的帐户具有与<mark>相关守护进程相匹配的名称</mark>：`osd.1` 或 `mgr.serverc`，并在安装期间创建。

使用 `librados` 的客户端应用所用帐户的名称具有 `client.` 前缀。例如，在将 OpenStack 与 Ceph 集成时，常常会创建专用的 `client.openstack` 用户帐户

管理员帐户的名称 `client.admin` ，在您运行管理性命令时，Ceph 会使用 `client.admin` 帐户，除非您通过 `--name` 或 `--id` 选项明确指定了用户名。

需要注意,--id不需要指定前缀，而--name需要前缀

```bash
[root@serverc ~]# ceph health --id admin
[root@serverc ~]# ceph health --name client.admin
```

<mark>Ceph 感知应用的最终用户不在 Ceph 集群中拥有帐户</mark>。他们会访问应用，然后由<mark>应用代表他们访问 Ceph</mark>。从 Ceph 角度来看，应用就是客户端。应用可能会通过其他机制提供自己的用户身份验证。

![](https://gitee.com/cnlxh/cl260/raw/master/images/component/configuring-config-auth.svg)

## Keyring 文件

为进行身份验证，客户端需要配置 Ceph 用户名，以及含有该用户机密密钥的密钥环文件。Ceph 会在用户帐户创建时，为每个用户帐户生成密钥环文件。

在运行ceph命令时，会<mark>自动加载</mark>以下位置的keyring  `/etc/ceph/$cluster.$name.keyring`，例如，对于 `client.openstack` 帐户，其密钥环文件为 `/etc/ceph/ceph.client.openstack.keyring`。如果keyring不在默认位置，就需要指定`--keyring`

客户端从监控器请求会话密钥。监控器会使用客户端的共享机密密钥加密会话密钥，再将会话密钥提供给客户端。由客户端解密会话密钥，然后再从监控器请求票据，以便对集群守护进程进行身份验证。这与 Kerberos 协议类似，cephx 密钥环文件与 Kerberos keytab 文件相当。

```bash
[root@serverc ~]# cat /etc/ceph/ceph.client.admin.keyring
[client.admin]
        key = AQA11VZhyq8VGRAAOus0I5xLWMSdAW/759e32A==
```

## 创建和查询Ceph用户

<mark>请注意，需要把keyring按照特定的格式放入特定的位置</mark>

```bash
[root@serverc ~]# ceph auth get-or-create client.lxh -o /etc/ceph/ceph.client.lxh.keyring
[root@serverc ~]# ceph auth get client.lxh
[client.lxh]
        key = AQCekPtkgKknMRAAtvgjIY8kTC1wjx/C8MIPIQ==
exported keyring for client.lxh
[root@serverc ~]# cat /etc/ceph/ceph.client.lxh.keyring
[client.lxh]
        key = AQCekPtkgKknMRAAtvgjIY8kTC1wjx/C8MIPIQ==
```

## 配置用户授权

您在创建新的用户帐户时，需授予足以授权用户集群任务的集群权限。`cephx` 中的权限称为*功能*，您通过守护进程类型（`mon`、`osd`、`mgr` 或 `mds`）来授予它们。

### Cephx Caps

在 `cephx` 内，对于每个守护进程类型来说，都有多项功能可用：

- `r` 授予读取访问权限。每一用户帐户至少应在监控器上具有读取访问权限，以便能够检索 CRUSH 映射。

- `w` 授予写入访问权限。客户端需要写入访问权限以在 OSD 上存储和修改对象。对于管理器 (MGR)，`w` 授予启用或禁用模块的权限。

- `x` 授予执行扩展对象类的权限。这使得客户端能够在对象上执行额外的操作，如使用 `rados lock get` 设置锁定或使用 `rbd list` 列出 RBD 镜像。

- `*` 授予完整访问权限。

- `class-read` 和 `class-write` 是 `x` 的子集。通常会用在 RBD 池中。

<mark>创建一个可以读取任何池数据的用户</mark>

```bash
[root@serverc ~]# ceph auth get-or-create client.lixiaohui mon 'allow r' osd 'allow rw'
[client.lixiaohui]
        key = AQCukftkgQf2KxAAir2uDa+zGv1m8VBNd3kvnQ==
[root@serverc ~]# ceph auth get client.lixiaohui
[client.lixiaohui]
        key = AQCukftkgQf2KxAAir2uDa+zGv1m8VBNd3kvnQ==
        caps mon = "allow r"
        caps osd = "allow rw"
exported keyring for client.lixiaohui
```

可使用配置文件来简化用户访问权限配置

```bash
[root@serverc ~]# ceph auth get-or-create client.zhangsan mon 'profile rbd' osd 'profile rbd'
[client.zhangsan]
        key = AQAYkvtk56CgIBAABi82zTgmAL01VeW6frIgYA==
[root@serverc ~]# ceph auth get client.zhangsan
[client.zhangsan]
        key = AQAYkvtk56CgIBAABi82zTgmAL01VeW6frIgYA==
        caps mon = "profile rbd"
        caps osd = "profile rbd"
exported keyring for client.zhangsan
```

配置文件还有很多，如下所示：

| 能力                      | 描述                                                         |
| ----------------------- | ---------------------------------------------------------- |
| `allow`                 | 在守护进程的访问权限设置之前。                                            |
| `r`                     | 为用户提供读取权限。需要用监控器来检索 CRUSH 映射。                              |
| `w`                     | 授予用户对对象的写入权限。                                              |
| `x`                     | 让用户能够调用类方法（即读取和写入）并对监控器执行身份验证操作。                           |
| `class-read`            | 让用户能够调用类读取方法。x 的子集。                                        |
| `class-write`           | 让用户能够调用类写入方法。x 的子集。                                        |
| `*`                     | 授予用户对特定守护进程或池的读取、写入和执行权限，并允许用户执行管理命令。                      |
| `profile osd`           | 授予用户作为 OSD 连接其他 OSD 或监控器的权限。授予 OSD 以便 OSD 能够处理复制心跳流量和状态报告。 |
| `profile bootstrap-osd` | 授予用户引导 OSD 的权限，以便他们在引导 OSD 时具有添加密钥的权限。                     |
| `profile rbd`           | 授予用户对 Ceph 块设备的读写权限。                                       |
| `profile rbd-read-only` | 授予用户对 Ceph 块设备的只读权限。                                       |

### 限制访问

下面的例子创建了lixiaohui用户，并给了他对lxhpool池的读写权限:

```bash
[root@serverc ~]# ceph auth get-or-create client.lixiaohui mon 'allow r' osd 'allow rw pool=lxhpool'
[client.lixiaohui]
        key = AQCvkvtkFO5bLhAA1qPAP8FJpDm963qyKB7y0Q==
[root@serverc ~]# ceph auth get client.lixiaohui
[client.lixiaohui]
        key = AQCvkvtkFO5bLhAA1qPAP8FJpDm963qyKB7y0Q==
        caps mon = "allow r"
        caps osd = "allow rw pool=lxhpool"
exported keyring for client.lixiaohui
```

**通过对象名称前缀**，下面的示例限制对任何池中名称以lxh开头的对象的访问

```bash
[root@serverc ~]# ceph auth get-or-create client.user2 mon 'allow r' osd 'allow rw object_prefix lxh'
[client.user2]
        key = AQD9kvtkFYmzORAAsJ5mKGXbojBuKHEJ2nY98A==
[root@serverc ~]# ceph auth get client.user2
[client.user2]
        key = AQD9kvtkFYmzORAAsJ5mKGXbojBuKHEJ2nY98A==
        caps mon = "allow r"
        caps osd = "allow rw object_prefix lxh"
exported keyring for client.user2
```

**通过namespace**，实现namespace来对池中的对象进行逻辑分组，然后可以将用户帐户限制为属于特定namespace的对象：

```bash
[root@serverc ~]# ceph auth get-or-create client.user3 mon 'allow r' osd 'allow rw namespace=lixiaohui'
[client.user3]
        key = AQAXk/tkcB9RHxAAwwzzTmc9cMVNH39NGNtjaA==
[root@serverc ~]# ceph auth get client.user3
[client.user3]
        key = AQAXk/tkcB9RHxAAwwzzTmc9cMVNH39NGNtjaA==
        caps mon = "allow r"
        caps osd = "allow rw namespace=lixiaohui"
exported keyring for client.user3
```

**通过路径**，Ceph文件系统(cepphfs)利用这种方法来限制对特定目录的访问，下面的例子创建了一个新的用户帐户user4，它只能访问/webcontent目录及其内容:

但这个要求你有一个运行中的文件系统

```bash
[root@serverc ~]# ceph fs volume create lxhfs
[root@serverc ~]# ceph fs authorize lxhfs client.user4 /webcontent rw
[client.user4]
        key = AQDVk/tkUBcTDRAA8LcsgCV92U3Walt1loWsFA==
[root@serverc ~]# ceph auth get client.user4
[client.user4]
        key = AQDVk/tkUBcTDRAA8LcsgCV92U3Walt1loWsFA==
        caps mds = "allow rw fsname=lxhfs path=/webcontent"
        caps mon = "allow r fsname=lxhfs"
        caps osd = "allow rw tag cephfs data=lxhfs"
exported keyring for client.user4
```

**通过monitor命令**，这种方法将管理员限制在特定的命令列表中，创建user5用户帐户并限制其访问两个命令的示例如下:

```bash
[root@serverc ~]# ceph auth get-or-create client.user5 mon 'allow r, allow command "auth get-or-create", allow command "auth list"'
[client.user5]
        key = AQAelPtkY5yZHBAAPV9UYuD5TdYzbJHD7CcG/g==
[root@serverc ~]# ceph auth get client.user5
[client.user5]
        key = AQAelPtkY5yZHBAAPV9UYuD5TdYzbJHD7CcG/g==
        caps mon = "allow r, allow command \"auth get-or-create\", allow command \"auth list\""
exported keyring for client.user5
```

## 用户管理命令汇总

ceph auth list 查询现有用户

```bash
[root@serverc ~]# ceph auth list | more
installed auth entries:

mds.fs1.serverc.scwhcl
        key: AQCFk/tkEVe6GBAAzMlgDnOn5VUIgLWc7MPOaw==
        caps: [mds] allow
        caps: [mon] profile mds
        caps: [osd] allow rw tag cephfs *=*
```

ceph auth get 获取特定帐户的详细信息

```bash
[root@serverc ~]# ceph auth get client.user2
[client.user2]
        key = AQD9kvtkFYmzORAAsJ5mKGXbojBuKHEJ2nY98A==
        caps mon = "allow r"
        caps osd = "allow rw object_prefix lxh"
exported keyring for client.user2
```

ceph auth print-key 显示用户密钥

```bash
[root@serverc ~]# ceph auth print-key client.user2
AQD9kvtkFYmzORAAsJ5mKGXbojBuKHEJ2nY98A==
```

ceph auth export、ceph auth import导出以及导入账号

```bash
[root@serverc ~]# ceph auth export client.user2 > user2.dat
export auth(key=AQD9kvtkFYmzORAAsJ5mKGXbojBuKHEJ2nY98A==)

[root@serverc ~]# ceph auth import -i user2.dat
imported keyring
```

ceph auth get-or-create 创建一个新用户帐户并生成它的密钥，通常会添加-o选项保存密钥，不然只是显示不保存

```bash
[root@serverc ~]# ceph auth get-or-create client.user7 mon 'allow r' osd 'allow rw' -o /etc/ceph/ceph.client.user7.keyring
[root@serverc ~]# ceph auth get client.user7
[client.user7]
        key = AQAHlftk+zzWBRAAByoEqAAQ7sfII62ivz/CQg==
        caps mon = "allow r"
        caps osd = "allow rw"
exported keyring for client.user7
```

ceph auth caps 修改用户Caps

将user6的osd中w权限去掉了，定义一个空字符串可以删除所有功能

```bash
[root@serverc ~]# ceph auth get client.user6
[client.user6]
        key = AQDvlPtk8ColExAARZUl1JhEVBaJjtcL+BJdDg==
        caps mon = "allow r"
        caps osd = "allow rw"
exported keyring for client.user6
[root@serverc ~]# ceph auth caps client.user6 mon 'allow r' osd 'allow r'
updated caps for client.user6
[root@serverc ~]# ceph auth get client.user6
[client.user6]
        key = AQDvlPtk8ColExAARZUl1JhEVBaJjtcL+BJdDg==
        caps mon = "allow r"
        caps osd = "allow r"
exported keyring for client.user6
```

ceph auth del 删除用户帐号

```bash
[root@serverc ~]# ceph auth del client.user6
updated

```
