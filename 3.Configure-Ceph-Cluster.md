```text
作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. 邮箱：939958092@qq.com
```

# 管理集群配置设置

## Ceph集群配置介绍

所有红帽 Ceph 存储集群配置都包含以下必要定义：

- 集群网络配置

- 集群监控器 (MON) 配置和引导选项

- 集群验证配置

- 守护进程配置选项

Ceph 配置设置使用唯一名称，该名称由<mark>下划线连接的小写字符</mark>组成。

每个Ceph守护进程都从以下来源访问它的配置：

- 编译中的默认值

- 集中配置数据库

- 存储在本地主机上的配置文件

- 环境变量

- 命令行参数

- 运行时覆盖

在新版本的Ceph里已经不再强调/etc/ceph/ceph.conf，<mark>集中配置数据库成为首选方式</mark>

## 修改集群配置文件

每个 Ceph 节点都会存储一个本地配置文件。集群配置文件的默认位置是 `/etc/ceph/ceph.conf`

配置文件采用 INI 文件格式，其中包含多个部分，内容涵盖 Ceph 守护进程和客户端配置。每个部分都有一个使用 `[name]` 标头定义的名称，以及定义为键值对的一个或多个参数。

```ini
[name] 
parameter1 = value1
parameter2 = value2 
```

### 配置部分

Ceph 使用所应用的守护进程或客户端的部分，将配置设置分组，确定是存储在配置文件中还是存储在配置数据库中。

- `[global]` 部分存储所有守护进程或读取配置的任何进程（包括客户端）所共有的一般配置。您可通过为单个守护进程或客户端创建调用部分来覆盖 `[global]` 参数。

- `[mon]` 部分存储监控器 (MON) 的配置。

- `[osd]` 部分存储 OSD 守护进程的配置。

- `[mgr]` 部分存储管理器 (MGR) 的配置。

- `[mds]` 部分存储元数据服务器 (MDS) 的配置。

- `[client]` 部分存储应用到所有 Ceph 客户客户端的配置。

### 实例设置

将适用于特定守护进程实例的设置归入各自部分，名称格式为 [*daemon-type*.*instance-ID*]

OSD 守护进程的实例 ID 始终为数字，例如 `[osd.0]`。客户端的实例 ID 是有效的用户名，例如 `[client.operator3]`

```bash
[mon]
# Settings for all mon daemons

[mon.serverc]
# Settings that apply to the specific MON daemon running on serverc
```

### 元变量

元变量是由 Ceph 定义的变量。您可使用它们来简化配置。

`$cluster`

红帽 Ceph 存储 5 集群的名称。默认集群名称为 `ceph`。

`$type`

守护进程类型，如监控器的值为 `mon`。OSD 使用 `osd`，元数据服务器使用 `mds`，管理器使用 `mgr`，客户端软件使用 `client`。

`$id`

守护进程实例 ID。对于此变量，`serverc` 上监控器的值为 `serverc`。`osd.1` 的 `$id` 值为 `1` ，客户端应用的值为用户名。

`$name`

守护进程名称和实例 ID。此变量是 `$type.$id` 的简写。

`$host`

运行守护进程的主机的名称。

## 使用集中式配置数据库

MON集群在MON节点上管理和存储集中配置数据库，可以临时更改设置(守护进程重新启动后丢失)，也可以配置设置永久保存并存储在数据库中，可以在集群运行时更改大多数配置设置

使用 `ceph config` 命令可查询数据库并查看配置信息。

- `ceph config ls` 可列出所有可能的配置设置。

- ``ceph config help setting`` 有助于进行特定配置设置。

- `ceph config dump` 可显示集群配置数据库设置。

- ``ceph config show $type.$id`` 可显示特定守护进程的数据库设置。使用 `show-with-defaults` 可包含默认设置。

- 使用 ``ceph config get $type.$id`` 可获得特定配置设置。

- 使用 ``ceph config set $type.$id`` 可设置特定配置设置。

使用 `assimilate-conf` 子命令可将文件中的配置应用到正在运行的集群。此过程将会识别配置文件中更改的设置并将其应用到集中式数据库。

```bash
[root@serverc ~]# ceph config assimilate-conf -i ceph.conf
```

## 在运行时覆盖配置设置

可以在守护进程运行时临时更改配置设置，守护进程重新启动时恢复到原来的设置：

`ceph tell $type.$id config` 命令临时覆盖配置设置，<mark>要求所配置的 MON 和守护进程都在运行</mark>

`ceph tell $type.$id config get `获取守护进程的特定运行时设置

`ceph tell $type.$id config set `设置守护进程的特定运行时设置

还可以接受通配符来获取或设置同一类型的所有守护进程的值。

例如cceph tell `osd.*` config get debug_ms 显示集群中所有OSD守护进程的该设置的值

`ceph daemon`<mark>不需要通过 MON 进行连接。即使 MON 未在运行</mark>

`ceph daemon $type.$id config `临时覆盖配置设置

## 查看现有配置

查看现有集中式数据库配置

```bash
[root@serverc ~]# ceph config dump
WHO                                       MASK  LEVEL     OPTION                                 VALUE
global                                          advanced  cluster_network                        172.25.249.0/24
global                                          basic     container_image                        registry.redhat.io/rhceph/rhceph-5-rhel8
  mon                                           advanced  auth_allow_insecure_global_id_reclaim  false
  mon                                           advanced  public_network                         172.25.250.0/24
  mgr                                           advanced  mgr/cephadm/container_init             True
  mgr                                           advanced  mgr/cephadm/migration_current          2
  mgr                                           advanced  mgr/cephadm/registry_password          redhat
  mgr                                           advanced  mgr/cephadm/registry_url               registry.redhat.io
  mgr                                           advanced  mgr/cephadm/registry_username          registry
  mgr                                           advanced  mgr/dashboard/ALERTMANAGER_API_HOST    http://172.25.250.12:9093
  mgr                                           advanced  mgr/dashboard/GRAFANA_API_SSL_VERIFY   false
  mgr                                           advanced  mgr/dashboard/GRAFANA_API_URL          https://172.25.250.12:3000
  mgr                                           advanced  mgr/dashboard/PROMETHEUS_API_HOST      http://172.25.250.12:9095
  mgr                                           advanced  mgr/dashboard/ssl_server_port          8443
  mgr                                           advanced  mgr/orchestrator/orchestrator          cephadm
    client.rgw.realm.zone.serverc.bqwjcv        basic     rgw_frontends                          beast port=80
    client.rgw.realm.zone.serverd.kfmflx        basic     rgw_frontends                          beast port=80
```

查看现有osd.1进程的配置

```basg
[root@serverc ~]# ceph config show osd.1
NAME                  VALUE                                                SOURCE    OVERRIDES  IGNORES
cluster_network       172.25.249.0/24                                       mon
container_image       registry.redhat.io/rhceph/rhceph-5-rhel8              mon
daemonize             false                                                 override
keyring               $osd_data/keyring                                     default
leveldb_log                                                                 default
log_stderr_prefix     debug                                                 default
log_to_file           false                                                 default
log_to_stderr         true                                                  default
mon_host              [v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0]     file
no_config_file        false                                                 override
rbd_default_features  61                                                    default
setgroup              ceph                                                  cmdline
setuser               ceph                                                  cmdline
```

## 永久修改数据库值

如果想要查看和修改目前的日志级别的值可以用下方的方法

看到目前的日志级别为0，Ceph 的日志级别从 1 到 20 ， 1 是简洁， 20 是详细，从上图来看0/0，第一个0是日志级别，第二个0是内存级别其实也是日志，只是把其暂存到内存中，现在我们需要把两者同时设置为1

```bash
[root@serverc ~]# ceph config show osd.1 debug_ms
0/0
[root@serverc ~]# ceph config show osd.1 debug_ms
0/0
[root@serverc ~]# ceph config set osd.1 debug_ms 1
[root@serverc ~]# ceph config show osd.1 debug_ms
1/1
[root@serverc ~]# ceph config get osd.1 debug_ms
1/1
```

重启一下osd.1的守护进程再次确认，我们发现这个值是被永久修改了

```bash
[root@serverc ~]# ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'serverc.lab.example.com'
[root@serverc ~]# ceph config get osd.1 debug_ms
1/1
[root@serverc ~]# ceph config show osd.1 debug_ms
1/1
```

## 临时修改数据库值

用ceph tell可以临时修改值，daemon重启后会恢复

```bash
[root@serverc ~]# ceph tell osd.1 config set debug_ms 10
{
    "success": ""
}
[root@serverc ~]# ceph tell osd.1 config get debug_ms
{
    "debug_ms": "10/10"
}
```

重启daemon再查看，发现其回到了我们永久修改的1

```bash
[root@serverc ~]# ceph orch daemon restart osd.1
Scheduled to restart osd.1 on host 'serverc.lab.example.com'
[root@serverc ~]# ceph tell osd.1 config get debug_ms
{
    "debug_ms": "1/1"
}
[root@serverc ~]# ceph config get osd.1 debug_ms
1/1
```

## 临时修改运行时参数

不建议这种方式，但集群故障无法运行时，可以考虑使用这种方式，或者你明白自己在做什么时使用，这种方式不要求集群正常，而且需要你登录到具体daemon所在的节点执行，在daemon重启后，所做的事将会恢复原来的样子

```bash
[ceph: root@serverc /]# ceph daemon osd.2 config get debug_ms
{
    "debug_ms": "0/0"
}
[ceph: root@serverc /]# ceph daemon osd.2 config set debug_ms 4
{
    "success": ""
}
[ceph: root@serverc /]# ceph daemon osd.2 config get debug_ms
{
    "debug_ms": "4/4"
}
[ceph: root@serverc /]# ceph orch daemon restart osd.2
Scheduled to restart osd.2 on host 'serverc.lab.example.com'
[ceph: root@serverc /]# ceph daemon osd.2 config get debug_ms
{
    "debug_ms": "0/0"
}
```

# 配置集群监控器

MON 将形成仲裁，采用 Paxos 算法的一种变体来选举*领导者*，以便在分布式计算机集之间达成一致。

各 MON 分别具有以下其中一个角色：

- *Leader*：第一个获得集群映射最新版本的 MON。

- *Provider*：拥有最新版本的集群映射，但不是 Leader 的 MON。

- *Requester*：没有最新版本的集群映射，必须先与提供商同步，然后才能重新加入仲裁的 MON。

要建立仲裁，集群中的<mark>大多数MONs必须处于运行状态</mark>，例如，如果部署了5个MONs，那么必须运行3个MONs来建立仲裁，在生产Ceph集群中部署<mark>至少三个MON节点</mark>，以确保高可用性，支持对运行中的集群添加或移除mon

集群配置文件会定义 MON 主机 IP 地址和要运行的集群的端口。该 `mon_host` 设置可以包含 IP 地址或 DNS 名称。`cephadm` 工具<mark>不会更新</mark>集群配置文件

```ini
[global]
        fsid = 6dbbdf24-33f1-11ed-84ff-000c29759605
        mon_host = [v2:192.168.30.130:3300/0,v1:192.168.30.130:6789/0] [v2:192.168.30.131:3300/0,v1:192.168.30.131:6789/0] [v2:192.168.30.132:3300/0,v1:192.168.30.132:6789/0]
```

## 查看Mon Map

Ceph集群map包括MON map、OSD map、PG map、MDS map和CRUSH map

MON映射包含集群fsid(文件系统ID)，以及与每个MON节点通信的名称、IP地址和网口。fsid是一个惟一的、自动生成的标识符(UUID)，用于标识Ceph集群

MON映射还保存映射版本信息，例如最后一次更改的序号(epoch )和时间，MON节点通过同步更改和对当前版本达成一致来维护映射

可以看到一共有4个投票的机器，serverc是仲裁的leader

```bash
[root@serverc ~]# ceph mon stat
e4: 4 mons at {clienta=[v2:172.25.250.10:3300/0,v1:172.25.250.10:6789/0],serverc.lab.example.com=[v2:172.25.250.12:3300/0,v1:172.25.250.12:6789/0],serverd=[v2:172.25.250.13:3300/0,v1:172.25.250.13:6789/0],servere=[v2:172.25.250.14:3300/0,v1:172.25.250.14:6789/0]}, election epoch 38, leader 0 serverc.lab.example.com, quorum 0,1,2,3 serverc.lab.example.com,clienta,serverd,servere
```

`json-pretty` 选项以创建更具可读性的输出 

```json
[root@serverc ~]# ceph quorum_status -f json-pretty

{
    "election_epoch": 38,
    "quorum": [
        0,
        1,
        2,
        3
    ],
    "quorum_names": [
        "serverc.lab.example.com",
        "clienta",
        "serverd",
        "servere"
    ],
    "quorum_leader_name": "serverc.lab.example.com",
    "quorum_age": 3500,
    "features": {
        "quorum_con": "4540138297136906239",
        "quorum_mon": [
            "kraken",
            "luminous",
            "mimic",
            "osdmap-prune",
            "nautilus",
            "octopus",
            "pacific",
            "elector-pinging"
        ]
    },
    "monmap": {
        "epoch": 4,
        "fsid": "2ae6d05a-229a-11ec-925e-52540000fa0c",
        "modified": "2021-10-01T09:33:53.880442Z",
        "created": "2021-10-01T09:30:30.146231Z",
        "min_mon_release": 16,
        "min_mon_release_name": "pacific",
        "election_strategy": 1,
        "disallowed_leaders: ": "",
        "stretch_mode": false,
        "features": {
            "persistent": [
                "kraken",
                "luminous",
                "mimic",
                "osdmap-prune",
                "nautilus",
                "octopus",
                "pacific",
                "elector-pinging"
            ],
            "optional": []
        },
        "mons": [
            {
                "rank": 0,
                "name": "serverc.lab.example.com",
                "public_addrs": {
                    "addrvec": [
                        {
                            "type": "v2",
                            "addr": "172.25.250.12:3300",
                            "nonce": 0
                        },
                        {
                            "type": "v1",
                            "addr": "172.25.250.12:6789",
                            "nonce": 0
                        }
                    ]
                },
                "addr": "172.25.250.12:6789/0",
                "public_addr": "172.25.250.12:6789/0",
                "priority": 0,
                "weight": 0,
                "crush_location": "{}"
            },
```

## 管理集中式配置数据库

MON节点存储和维护集中式配置数据库，数据库在每个MON节点上的默认位置是`/var/lib/ceph/$fsid/mon.$host/store.db`，<mark>不建议更改数据库的位置</mark>。

数据库会不断增大。运行 ``ceph tell mon.$id compact`` 命令可<mark>整合数据库，以提高性能</mark>。除此之外，也可将 `mon_compact_on_start` 配置设置为 `TRUE` ，以便在每次守护进程启动时压缩数据库：

```bash
[root@serverc ~]# ceph config set mon mon_compact_on_start true
```

根据数据库大小定义触发运行状况变化的阈值设置

| 描述                                                    | 设置                    | 默认值     |
| ----------------------------------------------------- | --------------------- | ------- |
| 当配置数据库的大小超过此值时，将集群运行状况更改为 `HEALTH_WARN`。              | `mon_data_size_warn`  | 15 (GB) |
| 当包含配置数据库的文件系统剩余容量小于或等于此百分比时，将集群运行状况更改为 `HEALTH_WARN`。 | `mon_data_avail_warn` | 30 (%)  |
| 当包含配置数据库的文件系统剩余容量小于或等于此百分比时，将集群运行状况更改为 `HEALTH_ERR`。  | `mon_data_avail_crit` | 5 (%)   |

## 压缩数据库文件

先查询现有大小

```bash
[root@serverc ~]# du -sch /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/store.db/
137M    /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/store.db/
137M    total
```

将mon 进程启动时压缩选项配置好，并重启mon进程，由于mon进程全部重启会导致集群状态异常，所以重启后要先确认集群状态

发现数据库压缩为74M

```bash
[root@serverc ~]# du -sch /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/
config           kv_backend       store.db/        unit.created     unit.meta        unit.run
keyring          min_mon_release  unit.configured  unit.image       unit.poststop    unit.stop

[root@serverc ~]# du -sch /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/store.db/
002126.sst       002128.sst       CURRENT          LOCK             OPTIONS-000014
002127.log       002129.sst       IDENTITY         MANIFEST-001982  OPTIONS-001985

[root@serverc ~]# du -sch /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/store.db/
137M    /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/store.db/
137M    total

[root@serverc ~]# ceph config set mon mon_compact_on_start true
[root@serverc ~]# ceph orch restart mon
Scheduled to restart mon.clienta on host 'clienta.lab.example.com'
Scheduled to restart mon.serverc.lab.example.com on host 'serverc.lab.example.com'
Scheduled to restart mon.serverd on host 'serverd.lab.example.com'
Scheduled to restart mon.servere on host 'servere.lab.example.com'

[root@serverc ~]# ceph health
HEALTH_OK

[root@serverc ~]# du -sch /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/
74M     /var/lib/ceph/2ae6d05a-229a-11ec-925e-52540000fa0c/mon.serverc.lab.example.com/
74M     total
```

# 配置集群网络

## 公共网络和集群网络

`public` 网络是所有 Ceph 集群通信的默认网络。`cephadm` 工具假定第一个 MON 守护进程 IP 地址的网络是 `public` 网络。新的 MON 守护进程部署在 `public` 网络中，除非您明确定义了不同的网络。

Ceph 客户端通过集群的 `public` 网络直接向 OSD 发送请求。OSD 复制和恢复流量会使用 `public` 网络，除非您为此配置了单独的 `cluster` 网络。

配置单独的 `cluster` 网络可能会减少 `public` 网络的流量负载并与后端 OSD 运维流量进行客户端分流，从而提高集群的性能。

![](https://gitee.com/cnlxh/cl260/raw/master/images/configure/networks-ceph-osd.svg)

通过执行以下步骤，配置单独 `cluster` 网络的节点。

- 在每个集群节点上配置一个额外的网络接口。

- 在每个节点的新网络接口上配置适当的 `cluster` 网络 IP 地址。

- 使用 `cephadm bootstrap` 命令的 `--cluster-network` 选项可在集群引导时创建 `cluster` 网络。

可使用 `ceph config set` 命令或 `ceph config assimilate-conf` 命令更改 `public` 和 `cluster` 网络。

## 查询目前网络情况

我们发现对于mon而言，需要在public暴露供客户端连接，而osd却只工作在cluster_network来保证安全

```bash
[root@serverc ~]# ceph config get mon public_network
172.25.250.0/24
[root@serverc ~]# ceph config get osd public_network

[root@serverc ~]# ceph config get osd cluster_network
172.25.249.0/24
[root@serverc ~]# ceph config get mon cluster_network
172.25.249.0/24
```

## 设置集群网络

很明显，mon是需要public网络来同步以及接受客户端请求的，但是osd如果也用这个网络，会有安全问题以及网络负载过高的问题，所以我们打算给他设置一个单独的集群网络，修改网络后需要重启集群生效

```bash
[root@serverc ~]# ceph config set mon public_network 172.25.250.0/24
[root@serverc ~]# ceph config set osd cluster_network 172.25.249.0/24
```

```bash
[root@serverc ~]# ceph config dump | grep cluster_network
global               advanced  cluster_network   172.25.249.0/24
osd                  advanced  cluster_network   172.25.249.0/24
```

## 启用巨型帧

配置网络 MTU 来支持巨型帧是存储网络中的推荐做法，也许可以提升性能。在 `cluster` 网络接口上配置 MTU 值 9000，以支持巨型帧。

通信路径中的所有节点和网络设备必须具有相同的 MTU 值。对于绑定网络接口，在绑定接口上设置 MTU 值，基础接口将继承相同的 MTU 值。

```bash
[root@serverc ~]# nmcli connection modify 'Wired connection 1' 802-3-ethernet.mtu 9000
[root@serverc ~]# nmcli connection modify 'Wired connection 2' 802-3-ethernet.mtu 9000
[root@serverc ~]# nmcli connection down 'Wired connection 1' ;nmcli connection up 'Wired connection 1'
[root@serverc ~]# nmcli connection down 'Wired connection 2' ;nmcli connection up 'Wired connection 2'
```

```bash
[root@serverc ~]# ip a s | grep 9000
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc fq_codel state UP group default qlen 1000
3: eth1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9000 qdisc fq_codel state UP group default qlen 1000
```

## 配置网络安全

配置集群网络之后，OSD的数据处理就不再对外开放，将后端OSD流量隔离到自己的网络中可能有助于防止public网络上的数据泄露，但是要注意的是，不要让public和cluster网络之间存在互通的可能性 

## 配置防火墙规则

Ceph OSD和MDS默认绑定的TCP端口范围为6800 ~ 7300。要配置不同的范围，需要修改ms_bind_port_min和ms_bind_port_max设置

**红帽 Ceph 存储服务的默认端口**

| 服务名称                    | 端口                               | 描述                                                                                                                |
| ----------------------- | -------------------------------- | ----------------------------------------------------------------------------------------------------------------- |
| 监控器 (MON)               | 6789/TCP (msgr)，3300/TCP (msgr2) | Ceph 集群内的通信                                                                                                       |
| OSD                     | 6800-7300/TCP                    | 每个 OSD 使用这个范围中的三个端口：一个用于通过公共网络与客户端和 MON 通信，一个用于通过集群网络或公共网络（如果前者不存在）发送数据到其他 OSD，另外一个用于通过集群网络或公共网络（如果前者不存在）交换心跳数据包。 |
| 元数据服务器 (MDS)            | 6800-7300/TCP                    | 与 Ceph 元数据服务器通信                                                                                                   |
| 控制面板/管理器 (MGR)          | 8443/TCP                         | 通过 SSL 与 Ceph 管理器控制面板通信                                                                                           |
| 管理器 RESTful 模块          | 8003/TCP                         | 通过 SSL 与 Ceph 管理器 RESTful 模块通信                                                                                    |
| 管理器 Prometheus 模块       | 9283/TCP                         | 与 Ceph 管理器 Prometheus 插件通信                                                                                        |
| Prometheus Alertmanager | 9093/TCP                         | 与 Prometheus Alertmanager 服务通信                                                                                    |
| Prometheus 节点导出器        | 9100/TCP                         | 与 Prometheus 节点导出器守护进程通信                                                                                          |
| Grafana 服务器             | 3000/TCP                         | 与 Grafana 服务通信                                                                                                    |
| Ceph 对象网关 (RGW)         | 80/TCP                           | 与 Ceph RADOSGW 通信。如果 `client.rgw` 配置部分为空， `cephadm` 会使用默认端口 `80`。                                                 |
| Ceph iSCSI 网关           | 9287/TCP                         | 与 Ceph iSCSI 网关通信                                                                                                 |

MON 始终在 `public` 网络上运行。若要按照防火墙规则保护 MON 节点，需使用 `public` 接口和 `public` 网络 IP 地址来配置规则

```bash
[root@serverc ~]# firewall-cmd --zone=public --add-port=6789/tcp
[root@serverc ~]# firewall-cmd --zone=public --add-port=6789/tcp --permanent 
```

还可通过添加 `ceph-mon` 服务至防火墙规则来保护 MON 节点。

```bash
[root@serverc ~]# firewall-cmd --zone=public --add-service=ceph-mon 
[root@serverc ~]# firewall-cmd --zone=public --add-service=ceph-mon --permanent 
```

若要配置 `cluster` 网络，OSD 同时需要 `public` 和 `cluster` 网络的规则。客户端通过使用公共网络连接到 OSD，OSD 通过集群网络互相通信。

若要按照防火墙规则保护 OSD 节点，需使用合适的网络接口和 IP 地址来配置规则。

```bash
[root@serverc ~]# firewall-cmd --zone=<public-or-cluster> --add-port=6800-7300/tcp 
[root@serverc ~]# firewall-cmd --zone=<public-or-cluster> --add-port=6800-7300/tcp --permanent 
```

还可通过添加 `ceph` 服务至防火墙规则来保护 OSD 节点。

```bash
[root@serverc ~]# firewall-cmd --zone=<public-or-cluster> --add-service=ceph 
[root@serverc ~]# firewall-cmd --zone=<public-or-cluster> --add-service=ceph --permanent
```
