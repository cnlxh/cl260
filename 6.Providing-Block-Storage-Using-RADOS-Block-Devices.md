```text
作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. 邮箱：939958092@qq.com
```

# 管理RADOS块设备

## RBD的块存储介绍

*RADOS 块设备* *(RBD)* 功能可从红 帽 Ceph 存储集群提供块存储。RADOS 提供虚拟块设备，并以 *RBD 镜像*形式存储在红帽 Ceph 存储集群的池中，<mark>在创建了存储池后，需要使用 `rbd pool init` 命令进行初始化赋予它rbd的功能。</mark>

`rbd_default_pool` 参数指定用于存储 RBD 镜像的<mark>默认池</mark>的名称，<mark>默认是rbd</mark>

## RBD 内核客户端介绍

内核 RBD 客户端 (`krbd`) 将 RBD 镜像映射到 Linux 块设备，Ceph 客户端可以使用原生 Linux 内核模块 (`krbd`) 挂载 RBD 镜像。这个模块使用 `/dev/rbd0` 之类的名称将 RBD 镜像映射到 Linux 块设备。

![](https://gitee.com/cnlxh/cl260/raw/master/images/block/devices-access-rbd-kernel.svg)

## 提供RDB块存储

### 创建存储池

首先需要创建一个池用于存放RDB镜像，然后将该池设置为RBD的应用程序类型

```bash
[root@serverc ~]# ceph osd pool create rbdpool
pool 'rbdpool' created
[root@serverc ~]# rbd pool init rbdpool
```

查询池的大小以及可用信息，发现我们目前rbdpool有32个PG，最大为28GiB

```bash
[root@serverc ~]# ceph df
--- RAW STORAGE ---
CLASS    SIZE   AVAIL     USED  RAW USED  %RAW USED
hdd    90 GiB  89 GiB  595 MiB   595 MiB       0.65
TOTAL  90 GiB  89 GiB  595 MiB   595 MiB       0.65

--- POOLS ---
POOL                   ID  PGS   STORED  OBJECTS     USED  %USED  MAX AVAIL
...
rbdpool                 8   32     19 B        1   12 KiB      0     28 GiB
```

### 安装ceph-common

 需要安装`ceph-common`来操作ceph

```bash
[root@serverc ~]# yum install ceph-common -y
```

### 创建客户端账号

创建一个名为`rbduse`的账号以供客户端来连接并使用此存储池，要注意需要把`keyring和ceph.conf`发给客户端

```bash
[root@serverc ~]# ceph auth get-or-create client.rbduse mon 'profile rbd' osd 'profile rbd' -o /etc/ceph/ceph.client.rbduse.keyring
```

### 分发配置文件和keyring

```bash
[ceph: root@host1 /]# scp /etc/ceph/ceph.conf /etc/ceph/ceph.client.rbduse.keyring root@clienta:/etc/ceph/
root@clienta's password: 
ceph.conf                             100%  339   533.3KB/s   00:00    
ceph.client.rbduse.keyring            100%   64   110.0KB/s   00:00    
```

### 创建RBD镜像

在rbdpool中，创建一个名为test切大小为500M的镜像

```bash
[root@serverc ~]# rbd create rbdpool/test --size=500M
[root@serverc ~]# rbd list -p rbdpool
test
```

### 查询RBD镜像参数

```bash
[root@serverc ~]# rbd info rbdpool/test
rbd image 'test':
        size 500 MiB in 125 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: d95ce194def6
        block_name_prefix: rbd_data.d95ce194def6
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Wed Aug  7 03:20:13 2024
        access_timestamp: Wed Aug  7 03:20:13 2024
        modify_timestamp: Wed Aug  7 03:20:13 2024
```

### 挂载RBD到客户端使用

客户端也需要安装`ceph-common`

```bash
[root@clienta ~]# yum install ceph-common -y
```

可以看到多了一个名为rbd0的500M的磁盘

```bash
[root@clienta ~]# rbd map rbdpool/test
/dev/rbd0
[root@clienta ~]# lsblk
NAME   MAJ:MIN RM  SIZE RO TYPE MOUNTPOINT
rbd0   251:0    0  500M  0 disk
```

查看RBD映射情况

```bash
[root@clienta ~]# rbd showmapped
id  pool     namespace  image  snap  device
0   rbdpool             test   -     /dev/rbd0
```

### 格式化挂载RBD设备

```bash
[root@clienta ~]# mkfs.xfs /dev/rbd0
meta-data=/dev/rbd0              isize=512    agcount=8, agsize=16000 blks
         =                       sectsz=512   attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=128000, imaxpct=25
         =                       sunit=16     swidth=16 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=1872, version=2
         =                       sectsz=512   sunit=16 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
Discarding blocks...Done.
```

我们挂载到/mnt上

```bash
[root@clienta ~]# mount /dev/rbd0 /mnt/
[root@clienta ~]# df -h | grep rbd0
/dev/rbd0       493M   29M  464M   6% /mnt
```

### 永久挂载

rbd的设备永久挂载一定要注意，这不是本地设备，需要ceph<mark>先操作把rbd0的设备映射成本地设备</mark>，然后才能本地挂载，不然你启动的时候挂载<mark>会卡住无法启动系统</mark>

先把rbd映射写好，并启动服务，如果不启用并启动服务，就需要每次系统启动后执行rbd map命令映射成本地设备，关机的时候执行rbd unmap

```bash
[root@clienta ~]# vim /etc/ceph/rbdmap
[root@clienta ~]# tail -n1 /etc/ceph/rbdmap
rbdpool/test id=rbduse,keyring=/etc/ceph/ceph.client.rbduse.keyring
[root@clienta ~]# systemctl enable rbdmap --now
```

写入fstab，并重启测试

<mark>请注意，这里fstab挂载请使用noauto</mark>

重启后，发现rbd0还是挂载的

```bash
[root@client ~]# vim /etc/fstab 
[root@client ~]# tail -n1 /etc/fstab
/dev/rbd0 /mnt xfs defaults,noauto 0 0 
[root@client ~]# reboot
[root@clienta ~]# df -h | grep mnt
/dev/rbd0       493M   29M  464M   6% /mnt
```

### 写入数据

```bash
[root@clienta ~]# dd if=/dev/zero of=/mnt/test.data bs=10M count=20
20+0 records in
20+0 records out
209715200 bytes (210 MB, 200 MiB) copied, 0.111759 s, 1.9 GB/s
```

### 查询rados数据

```bash
[root@clienta ~]# rbd info rbdpool/test
rbd image 'test':
        size 500 MiB in 125 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: d95ce194def6
        block_name_prefix: rbd_data.d95ce194def6
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Wed Aug  7 03:20:13 2024
        access_timestamp: Wed Aug  7 03:20:13 2024
        modify_timestamp: Wed Aug  7 03:20:13 2024
[root@clienta ~]# rados -p rbdpool ls
rbd_data.d95ce194def6.000000000000005f
rbd_data.d95ce194def6.0000000000000044
rbd_data.d95ce194def6.0000000000000064
rbd_data.d95ce194def6.0000000000000000
rbd_data.d95ce194def6.000000000000006e
```

### 查询RBD镜像实际使用量

```bash
[root@clienta ~]# rbd disk-usage rbdpool/test
NAME  PROVISIONED  USED
test      500 MiB  472 MiB
```

### RBD 镜像大小扩容

如果RBD随着后期的需求增长，不够大了怎么办，那就需要使RBD变大，目前我们只有500M，什么都放不下

```bash
[root@clienta ~]# rbd disk-usage -p rbdpool
NAME  PROVISIONED  USED
test      500 MiB  472 MiB
```

扩大到10G

```bash
[root@clienta ~]# rbd resize rbdpool/test --size 10G
Resizing image: 100% complete...done.
[root@clienta ~]#
[root@clienta ~]# rbd disk-usage -p rbdpool
NAME  PROVISIONED  USED
test       10 GiB  472 MiB
[root@clienta ~]# xfs_growfs /mnt/
...
data blocks changed from 128000 to 2621440

[root@clienta ~]# df -h | grep mnt
/dev/rbd0        10G  306M  9.7G   3% /mnt
```

### RADOS块设备镜像布局

RBD 镜像中的所有对象的名称以各个 RBD 镜像 RBD 块名称前缀字段中包含的值为开头，这可使用 `rbd info` 命令来显示。此前缀后是句点 (.)，后跟对象编号。对象编号字段的值是 12 个字符的十六进制数字。

```bash
[root@clienta ~]# rbd info rbdpool/test
rbd image 'test':
        size 10 GiB in 2560 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: d95ce194def6
        block_name_prefix: rbd_data.d95ce194def6
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Wed Aug  7 03:20:13 2024
        access_timestamp: Wed Aug  7 03:20:13 2024
        modify_timestamp: Wed Aug  7 03:20:13 2024
```

RBD 镜像基于对象分条，并存储在 RADOS 对象存储中

![](https://gitee.com/cnlxh/cl260/raw/master/images/block/devices-rbd-layout.svg)

# 管理RADOS块设备快照与克隆

## RBD 快照概述

<mark>*RBD 快照*</mark>是创建于特定时间的 <mark>RBD 镜像的只读副本</mark>。RBD 快照使用<mark> COW 技术</mark>来最大程度减少维护快照所需的存储空间。在将写入 I/O 请求应用到 RBD 快照镜像前，集群会将原始数据复制到 I/O 操作所影响对象的 PG 中的另一区域。快照在创建时不会占用存储空间，但会随着所包含对象的变化而增大。<mark>RBD 镜像支持增量快照</mark>。

从下图看，RBD的快照并不会占用太大的空间

![](https://gitee.com/cnlxh/cl260/raw/master/images/block/device-snapshot01.svg)

COW图示

![](https://gitee.com/cnlxh/cl260/raw/master/images/block/device-snapshot02.svg)

**在拍摄快照前，使用 `fsfreeze` 命令暂停对文件系统的访问。`fsfreeze` 命令可以停止对文件系统的访问并在磁盘上创建稳定的镜像。不要在文件系统未冻结时拍摄文件系统快照，因为这样会损坏快照的文件系统。拍摄快照后，使用 `fsfreeze` 命令可恢复文件系统操作和访问。**

## 创建快照

现在Ceph的镜像默认是版本2，支持layering快照克隆

创建Ceph设备的快照，给rbdpool中的test创建一个名为snap1的快照

先冻结文件系统访问，然后拍摄快照，然后再解除冻结

```bash
[root@clienta ~]# fsfreeze --freeze /mnt
[root@clienta ~]# rbd snap create rbdpool/test@snap1
Creating snap: 100% complete...done.
[root@clienta ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE    PROTECTED  TIMESTAMP
     4  snap1  10 GiB             Wed Aug  7 03:47:49 2024
[root@clienta ~]# fsfreeze --unfreeze /mnt
```

## 恢复数据

先把数据删除，然后挂载测试恢复后数据是否回来

```bash
[root@clienta ~]# ls /mnt
test.data
[root@clienta ~]# rm -rf /mnt/*
[root@clienta ~]# ls /mnt/
[root@clienta ~]# umount /mnt
[root@clienta ~]# rbd unmap rbdpool/test
[root@clienta ~]# rbd snap rollback rbdpool/test@snap1
Rolling back to snapshot: 100% complete...done.
```

挂载后发现数据已经恢复成功

```bash
[root@clienta ~]# rbd map rbdpool/test
/dev/rbd0
[root@clienta ~]# mount /dev/rbd0 /mnt/
[root@clienta ~]# ls /mnt/
test.data
```

## 删除快照

```bash
[root@clienta ~]# rbd snap rm rbdpool/test@snap1
Removing snap: 100% complete...done.
```

一个一个删除麻烦，如果需要，可以一起清空

```bash
[root@clienta ~]# rbd snap purge rbdpool/test
Removing all snapshots: 100% complete...done.
```

## 克隆与快照技术对比

快照就是针对RBD某个时间点状态的复制，以后出了问题，可以回到那个时间点，这么说来，快照是只读的，而克隆一般是基于某个快照，默认启用的是写时复制技术，<mark>克隆可以看做是一个可读可写的RBD镜像</mark>，其创建之初，并不占用任何物理空间，你在写入或修改某些东西时，从其基于的快照复制过来，然后修改并保存，此时才占用空间，当然，也可以用命令直接<mark>斩断其和上级的关系，成为一个独立的RBD镜像</mark>，此时会把所有缺失的内容从上级拷贝过来。

克隆支持 COW 和 COR，默认为 COW

## 创建克隆

创建克隆一般有三个步骤

1. 创建快照

2. 保护快照不被删除

3. 创建克隆

我们重新创建名为snap1的快照，基于它，我们创建一个clone1的克隆

```bash
[root@clienta ~]# touch /mnt/lxh
[root@clienta ~]# ls /mnt
lxh
[root@clienta ~]# fsfreeze --freeze /mnt
[root@clienta ~]# rbd snap create rbdpool/test@snap1
Creating snap: 100% complete...done.
[root@clienta ~]# fsfreeze --unfreeze /mnt
```

```bash
[root@clienta ~]# rbd snap protect rbdpool/test@snap1
[root@clienta ~]# rbd clone rbdpool/test@snap1 rbdpool/clone1
[root@clienta ~]# rbd ls -p rbdpool
clone1
test
```

查询快照占用的空间，我们发现其实际上没有占用空间

```bash
[root@clienta ~]# rbd disk-usage -p rbdpool
NAME        PROVISIONED  USED
clone1           10 GiB      0 B
test@snap1       10 GiB  1.1 GiB
test             10 GiB  1.1 GiB
<TOTAL>          20 GiB  2.1 GiB
```

我们可以用info来看到它的父级关系

parent

```bash
[root@clienta ~]# rbd info rbdpool/clone1
rbd image 'clone1':
        size 10 GiB in 2560 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: faade2ee4808
        block_name_prefix: rbd_data.faade2ee4808
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Wed Aug  7 04:01:15 2024
        access_timestamp: Wed Aug  7 04:01:15 2024
        modify_timestamp: Wed Aug  7 04:01:15 2024
        parent: rbdpool/test@snap1
        overlap: 10 GiB
```

## 使用克隆

我们将克隆映射到客户端本地，完成读写验证，并确认其磁盘占用空间变大

要挂克隆到本地，你<mark>需要卸载前面挂载的数据卷</mark>，挂载克隆后，我们发现其数据还在

```bash
[root@clienta ~]# umount /mnt
[root@clienta ~]# ls /mnt
[root@clienta ~]# rbd map rbdpool/clone1
/dev/rbd1
[root@clienta ~]# mount /dev/rbd1 /mnt
[root@clienta ~]# ls /mnt
lxh
```

```bash
[root@client ~]# rbd disk-usage -p rbdpool
NAME        PROVISIONED  USED   
clone1          500 MiB   12 MiB
test@snap1      500 MiB  136 MiB
test            500 MiB  476 MiB
<TOTAL>        1000 MiB  624 MiB
```

## 扁平化克隆

将数据从快照中合并到我们的克隆在扁平化克隆时，Ceph 会将所有缺失的数据从父级复制到克隆，然后移除对父级的引用。克隆会变成独立的 RBD 镜像，不再是受保护快照的子级。

```bash
[root@clienta ~]# rbd flatten rbdpool/clone1
Image flatten: 100% complete...done.
[root@clienta ~]# rbd disk-usage -p rbdpool
NAME        PROVISIONED  USED
clone1           10 GiB  660 MiB
test@snap1       10 GiB  1.1 GiB
test             10 GiB  1.1 GiB
<TOTAL>          20 GiB  2.8 GiB
```

此时再看，就发现已经不再存在父级关系，成为了独立的镜像

```bash
[root@clienta ~]# rbd info rbdpool/clone1
rbd image 'clone1':
        size 10 GiB in 2560 objects
        order 22 (4 MiB objects)
        snapshot_count: 0
        id: fac9611f2d3
        block_name_prefix: rbd_data.fac9611f2d3
        format: 2
        features: layering, exclusive-lock, object-map, fast-diff, deep-flatten
        op_features:
        flags:
        create_timestamp: Wed Aug  7 04:07:12 2024
        access_timestamp: Wed Aug  7 04:07:12 2024
        modify_timestamp: Wed Aug  7 04:07:12 2024
```

# 导入和导出RBD镜像

## 导入和导出RBD镜像

利用 RBD 导出与导入机制，您可以在同一集群中，也可以使用另一独立集群维护拥有完整功能和可访问性的 RBD 镜像操作副本。您可以将这些副本用于各种不同的用例，包括：

- 利用实际的数据卷测试新版本

- 利用实际的数据卷运行质量保障流程

- 实施业务连续性方案

- 将备份进程从生产块设备分离

RADOS 块设备功能支持导出和导入整个 RBD 镜像，也可以仅导出和导入两个时间点之间的 RBD 镜像变化。

### 导出RBD镜像

 `rbd export` 命令将 RBD 镜像导出到文件。此命令可以将 RBD 镜像或 RBD 镜像快照导出到指定的目标文件

```bash
[root@clienta ~]# rbd export rbdpool/test export.dat
Exporting image: 100% complete...done.
[root@clienta ~]# ll -h export.dat
-rw-r--r--. 1 root root 10G Aug  7 04:14 export.dat
```

### 导入RBD镜像

Ceph Storage提供了rbd import命令，用于从文件中导入rbd镜像。该命令<mark>创建一个test2新镜像</mark>

```bash
[root@clienta ~]# rbd import export.dat rbdpool/test2
Importing image: 100% complete...done.
[root@clienta ~]# rbd ls -p rbdpool
clone1
test
test2
```

## 导出和导入RBD镜像的变化部分

 `rbd export-diff` 和 `rbd import-diff` 命令来导出和导入在两个时间点之间对 RBD 镜像的变更

端点时间可以是：

- RBD 镜像的当前内容，如 `poolname/imagename`。

- RBD 镜像的快照，如 `poolname/imagename@snapname`。

起始时间可以是：

- RBD 镜像的创建日期和时间。例如，不使用 `--from-snap` 选项。

- RBD 镜像的快照，如通过 `--from-snap snapname` 选项获取。

如果您指定起始点快照，命令将导出您创建该快照后的更改。如果不指定快照，命令将导出自 RBD 镜像创建之后的所有更改，这与常规的 RBD 镜像导出操作相同。

### 导出两个快照的不同之处

我们已经有一个包含数据的snap1，我们再向test镜像中写入点内容，然后重做一个新快照

```bash
[root@clienta ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE    PROTECTED  TIMESTAMP
    10  snap1  10 GiB  yes        Wed Aug  7 04:06:46 2024
[root@clienta ~]# rbd map rbdpool/test
/dev/rbd0
[root@clienta ~]# mount /dev/rbd0 /mnt
mount: /mnt: /dev/rbd0 already mounted on /mnt.
[root@clienta ~]# dd if=/dev/zero of=/mnt/test2.data bs=10M count=1
1+0 records in
1+0 records out
10485760 bytes (10 MB, 10 MiB) copied, 0.0199916 s, 525 MB/s
[root@clienta ~]# ls /mnt
lxh  test2.data
[root@clienta ~]# fsfreeze --freeze /mnt
[root@clienta ~]# rbd snap create rbdpool/test@snap2
Creating snap: 100% complete...done.
[root@clienta ~]# fsfreeze --unfreeze /mnt
[root@clienta ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE    PROTECTED  TIMESTAMP
    10  snap1  10 GiB  yes        Wed Aug  7 04:06:46 2024
    11  snap2  10 GiB             Wed Aug  7 04:30:54 2024
```

导出其不同之处，这就相当于做了一个增量备份

<mark>导出的是snap1到snap2的增量</mark>

```bash
[root@clienta ~]# rbd export-diff --from-snap snap1 rbdpool/test@snap2 diff.dat
Exporting image: 100% complete...done.
[root@clienta ~]# ll -h diff.dat
-rw-r--r--. 1 root root 2.3M Aug  7 04:31 diff.dat
```

### 导入这个增量数据到镜像

不要随便导入这个增量数据，除非你发生了数据丢失，比如snap2不见了

```bash
[root@clienta ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE    PROTECTED  TIMESTAMP
    10  snap1  10 GiB  yes        Wed Aug  7 04:06:46 2024
    13  snap2  10 GiB             Wed Aug  7 04:33:18 2024
[root@clienta ~]# rbd snap rm rbdpool/test@snap2
Removing snap: 100% complete...done.
[root@clienta ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE    PROTECTED  TIMESTAMP
    10  snap1  10 GiB  yes        Wed Aug  7 04:06:46 2024
[root@clienta ~]# rbd import-diff ./diff.dat rbdpool/test
Importing image diff: 100% complete...done.
[root@clienta ~]# rbd snap ls rbdpool/test
SNAPID  NAME   SIZE    PROTECTED  TIMESTAMP
    10  snap1  10 GiB  yes        Wed Aug  7 04:06:46 2024
    15  snap2  10 GiB             Wed Aug  7 04:33:38 2024
```

### 合并差异

写入点数据，然后再做一个增量备份

导出的是snap2到实时的增量

```bash
[root@clienta ~]# touch /mnt/diff2
[root@clienta ~]# ls /mnt/
diff2  lxh  test2.data
[root@clienta ~]# rbd export-diff --from-snap snap2 rbdpool/test diff2.dat
Exporting image: 100% complete...done.
```

合并两次差异

```bash
[root@clienta ~]# rbd merge-diff diff.dat diff2.dat mergeresult.dat
Merging image diff: 100% complete...done.
```
