```text
作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. 邮箱：939958092@qq.com 
```

# 部署共享文件存储

## Ceph文件系统和MDS

基于文件的存储可以像传统文件系统一样整理您的数据，并<mark>带有目录树层次结构</mark>。实施 Ceph 文件系统需要一个正在运行的 Ceph 存储集群和至少一台 *Ceph 元数据服务器 (MDS)* 来管理 CephFS 元数据，并与文件数据分隔开，这样可以降低复杂性并提高可靠性。

### 文件、块和对象存储

<mark>基于文件的存储</mark>使用目录树层次结构来组织文件的存储方式，<mark>直接提供空间，而无需格式化</mark>

<mark>基于块</mark>的存储提供了一个存储卷，它的操作类似于<mark>磁盘设备</mark>，组织成大小相同的块，<mark>需要格式化使用</mark>。

通过<mark>基于对象的存储</mark>，可将任意数据和元数据作为一个标有唯一标识符的单元存储在<mark>扁平存储池</mark>中。您会<mark>使用 API 来存储和检索数据</mark>，而不会作为块或在文件系统层次结构中访问数据。

### 元数据服务器

元数据服务器 (MDS) <mark>管理 CephFS 客户端的元数据</mark>。该守护进程会提供 CephFS 客户端访问 RADOS 对象所需的信息，如<mark>提供文件在文件系统中的位置</mark>。<mark>MDS 负责管理目录层次结构</mark>并在 RADOS 集群中存储文件元数据（如所有者、时间戳和权限模式）。

MDS 守护进程以两种模式运行，分别为：<mark>活动和备用</mark>。活动 MDS 负责管理 CephFS 文件系统上的元数据。备用 MDS 充当备份，并会在活动 MDS 无响应时切换到活动模式。

CephFS 客户端首先会联系 MON 进行身份验证并检索集群映射。然后，<mark>客户端会查询活动 MDS 以获取文件元数据</mark>。客户端通过<mark>直接与 OSD 通信</mark>，使用元数据来访问组成所请求文件或目录的对象。

### MDS 自动缩放器

CephFS 共享文件系统至少需要一个活动 MDS 服务才能正确运行，还至少需要一个备用 MDS 才能确保高可用性。MDS 自动缩放器模块可确保有足够多的 MDS 守护进程可用。

```bash
[root@serverc ~]# ceph mgr module enable mds_autoscaler
```

## 部署CephFS

要实施 CephFS 文件系统，需完成以下步骤

1. 创建所需池

2. 创建 CephFS 文件系统

3. 部署 MDS 守护进程

4. 然后挂载该文件系统。

以上步骤可手工完成每一步，也可使用 `ceph fs volume create` 命令<mark>自动执行所有这些步骤</mark>，只是手工操作可以管理细节 

### 使用卷方法创建 CephFS

使用 `ceph fs volume` 直接创建 CephFS 卷。此命令可创建与 CephFS 关联的池，创建 CephFS 卷，还可在主机上启动 MDS 服务。

```bash
[root@serverc ~]# ceph fs volume create lxhfsname --placement='serverc.lab.example.com'
[root@serverc ~]# ceph orch ls
NAME                     RUNNING  REFRESHED  AGE   PLACEMENT
mds.lxhfsname                1/1  -          3s    serverc.lab.example.com
```

### 手工创建CephFS

CephFS 文件系统需要至少两个池，一个用于存储 CephFS 数据，另一个用于存储 CephFS 元数据

```bash
[root@serverc ~]# ceph osd pool create fs_data
pool 'fs_data' created
[root@serverc ~]# ceph osd pool create fs_metadata
pool 'fs_metadata' created
```

创建一个文件系统

```bash
[root@serverc ~]# ceph fs new mdsfs fs_metadata fs_data
new fs with metadata pool 14 and data pool 13
```

部署mds进程

```bash
[root@serverc ~]# ceph orch apply mds mdsfs --placement='1 serverc.lab.example.com'
Scheduled mds.mdsfs update...
```

查看文件系统情况

```bash
[root@serverc ~]# ceph fs status
mdsfs - 2 clients
=====
RANK  STATE           MDS              ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  mdsfs.serverc.xwoxuu  Reqs:    0 /s    10     13     12      2
    POOL       TYPE     USED  AVAIL
fs_metadata  metadata  96.0k  28.3G
  fs_data      data       0   28.3G
MDS version: ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)
```

### 使用服务规范创建 CephFS

先部署进程，后创建文件系统也可以

```yaml
service_type: mds
service_id: mdsfs
placements:
  hosts:
    - serverc.lab.example.com
```

```bash
ceph orch apply -i file-name.yml
```

创建一个文件系统

```bash
[root@serverc ~]# ceph fs new mdsfs fs_metadata fs_data
new fs with metadata pool 14 and data pool 13
```

查看文件系统情况

```bash
[root@serverc ~]# ceph fs status
mdsfs - 2 clients
=====
RANK  STATE           MDS              ACTIVITY     DNS    INOS   DIRS   CAPS
 0    active  mdsfs.serverc.xwoxuu  Reqs:    0 /s    10     13     12      2
    POOL       TYPE     USED  AVAIL
fs_metadata  metadata  96.0k  28.3G
  fs_data      data       0   28.3G
MDS version: ceph version 16.2.0-117.el8cp (0e34bb74700060ebfaa22d99b7d2cdc037b28a57) pacific (stable)
```

## 挂载文件系统

可以使用任一可用的客户机挂载CephFS文件系统:

1. 内核客户端：不支持配额，但速度可能较快，仅支持内核版本达到 4 或以上

2. FUSE客户端：支持配额和 ACL，不要求新内核版本

### 常见 CephFS 客户端配置

要使用任一客户端挂载基于 CephFS 的文件系统，请验证客户端主机是否满足下列前提条件。

- 安装 `ceph-common` 软件包。对于 FUSE 客户端，还要安装 `ceph-fuse` 软件包。

- 验证 Ceph 配置文件是否存在（默认为 `/etc/ceph/ceph.conf`）。

- 授权客户端访问 CephFS 文件系统。

- 使用 `ceph auth get` 命令提取新的授权密钥，并将其复制到客户端主机上的 `/﻿etc/﻿ceph` 文件夹。

- 以非 root 用户身份使用 FUSE 客户端时，需在 `/etc/fuse.conf` 配置文件中添加 `user_allow_other`。

### 使用FUSE客户端挂载cephfs

先安装fuse客户端

```bash
[root@clienta ~]# yum install ceph-fuse -y
```

确认配置文件和keyring

```bash
[root@clienta ~]# ll /etc/ceph/
total 12
-rw-r--r--. 1 root root  63 Oct  1  2021 ceph.client.admin.keyring
-rw-r--r--. 1 root root 177 Oct  1  2021 ceph.conf
```

挂载文件系统

```bash
[root@clienta ~]# ceph-fuse -n client.admin --client_fs mdsfs /mnt/
2024-08-08T05:01:17.033-0400 7f2c5fde3200 -1 init, newargv = 0x561dc467b910 newargc=15
ceph-fuse[38129]: starting ceph client
ceph-fuse[38129]: starting fuse
[root@clienta ~]# df -h | grep mnt
ceph-fuse        29G     0   29G   0% /mnt
```

永久挂载

```bash
[root@clienta ~]# tail -n1 /etc/fstab
serverc.lab.example.com:/ /mnt fuse.ceph ceph.id=admin,_netdev 0 0
```

### 用内核客户端安装CephFS

在客户端节点上安装 ceph-common 软件包

```bash
[root@clienta ~]# yum install ceph-common -y
```

确认配置文件和keyring

```bash
[root@clienta ~]# ll /etc/ceph/
total 12
-rw-r--r--. 1 root root  63 Oct  1  2021 ceph.client.admin.keyring
-rw-r--r--. 1 root root 177 Oct  1  2021 ceph.conf
```

挂载文件系统

```bash
[root@clienta ~]# mount.ceph serverc.lab.example.com:/ /opt -o name=admin
[root@clienta ~]# df -h | grep opt
172.25.250.12:/   29G     0   29G   0% /opt

[root@clienta ~]# mount -l | grep opt
172.25.250.12:/ on /opt type ceph (rw,relatime,seclabel,name=admin,secret=<hidden>,acl)
```

永久挂载

```bash
[root@clienta ~]# tail -n1 /etc/fstab
serverc.lab.example.com:/ /opt ceph name=admin,_netdev 0 0
```

## CephFS的访问控制

通过 `ceph fs authorize` 命令，您可为 CephFS 文件系统中的不同用户和文件夹提供精细的访问控制。您可为 CephFS 文件系统中的文件夹设置不同选项：

- `r`：对指定文件夹的读取权限。如果未指定其他限制，则会向子文件夹授予读取权限。

- `w`：对指定文件夹的写入权限。如果未指定其他限制，则会向子文件夹授予写入权限。

- `p`：除了 `r` 和 `w` 功能外，客户端还需要 `p` 选项才能使用布局或配额。

- `s`：除了 `r` 和 `w` 功能外，客户端还需要 `s` 选项才能创建快照。

本例允许client.lxh-fs-user读取根文件夹，并提供对 `/directory` 文件夹的读取、写入和快照权限。

这里会自动创建出用户

```bash
[root@serverc ~]# ceph fs authorize mdsfs client.lxh-fs-user / r /directory rws
[client.lxh-fs-user]
        key = AQCHjbRma0dEBRAAk7BbMTLYjhzsR1+ZhwB2rw==
```

## 删除CephFS

如果需要，可以删除CephFS。但是，首先要备份所有数据，因为删除CephFS文件系统会破坏该文件系统上存储的所有数据。

要删除 CephFS，首先要将其标记为 down

```bash
[root@serverc ~]# ceph fs set mdsfs down true
mdsfs marked down.
```

然后，可以使用下一个命令删除它

```bash
[root@serverc ~]# ceph fs rm mdsfs --yes-i-really-mean-it
```

## Ceph实施NFS集群

### 创建文件系统

```bash
[root@serverc ~]# ceph fs volume create lxhfsname-2 --placement='serverc.lab.example.com'
```

### 在MGR上安装必备软件

在 Ceph MGR 节点上安装 nfs-ganesha、nfs-ganesha-ceph、nfs-ganesha-rados-grace 和 nfs-ganesha-rados-urls 软件包

```bash
[root@serverc ~]# yum install nfs-ganesha nfs-ganesha-ceph nfs-ganesha-rados-grace nfs-ganesha-rados-urls -y
```

### 开启nfs模块

```bash
[root@serverc ~]# ceph mgr module enable nfs
```

### 创建nfs集群

```bash
[root@serverc ~]# ceph nfs cluster create nfsclustername 'serverc.lab.example.com,servere.lab.example.com'
NFS Cluster Created Successfully

[root@serverc ~]# ceph nfs cluster info
{
    "nfsclustername": [
        {
            "hostname": "serverc.lab.example.com",
            "ip": "172.25.250.12",
            "port": 2049
        }
    ]
}
```

### 导出 CephFS 文件系统

格式是：

```bash
ceph nfs export create cephfs <fsname> <clusterid> <binding> [--readonly] [<path>] 
```

我在nfs集群中，创建了/nfs1的路径

```bash
[root@serverc ~]# ceph nfs export create cephfs nfsclustername lxhfsname-2 /nfs1
{
    "bind": "/nfs1",
    "fs": "lxhfsname-2",
    "path": "/",
    "cluster": "nfsclustername",
    "mode": "RW"
}
```

查看具体的导出详情

```bash
[root@serverc ~]# ceph nfs export ls nfsclustername
[
  "/nfs1"
]

[root@serverc ~]# ceph nfs export info nfsclustername /nfs1

{
  "access_type": "RW",
  "clients": [],
  "cluster_id": "nfsclustername",
  "export_id": 1,
  "fsal": {
    "fs_name": "lxhfsname-2",
    "name": "CEPH",
    "user_id": "nfs.nfsclustername.1"
  },
  "path": "/",
  "protocols": [
    4
  ],
  "pseudo": "/nfs1",
  "security_label": true,
  "squash": "none",
  "transports": [
    "TCP"
  ]
}
```

### 开启防火墙

```bash
[root@serverc ~]# firewall-cmd --add-port=2049/tcp --permanent
success
[root@serverc ~]# firewall-cmd --reload
success
```

### 客户端挂载nfs

先安装nfs客户端

```bash
[root@clienta ~]# yum install nfs-utils -y
```

```bash
[root@clienta ~]# mount -t nfs serverc:/nfs1 /mnt
[root@clienta ~]# df -h | grep mnt
clienta:/nfs1          190G     0  190G   0% /mnt
```
