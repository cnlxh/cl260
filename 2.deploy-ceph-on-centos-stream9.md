| 编号  | 系统         | 角色               | IP             | Ceph版本 |硬盘数量|
| --- | ---------- | ---------------- | -------------- | ------ | ------ |
| 1   | Centos stream 9 | 引导节点，mon，mgr，osd,rgw | 192.168.8.21 | Reef |4|
| 2   | Centos stream 9 | mon，mgr，osd | 192.168.8.22 | Reef |4|
| 3   | Centos stream 9 | mon，mgr，osd | 192.168.8.23 | Reef |4|


作者：李晓辉

联系方式：

1. 微信：Lxh_Chat

2. 邮箱：xiaohui_li@foxmail.com

# 先决条件准备

除非另有说明，不然先决条件需在每个参与节点准备

1. Python 3
2. Systemd
3. Podman 3
4. Chrony
5. LVM2

准备好集群之间的解析

```bash
cat > /etc/hosts <<-'EOF'
127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4
::1         localhost localhost.localdomain localhost6 localhost6.localdomain6
192.168.8.21 ceph
192.168.8.22 ceph-2
192.168.8.23 ceph-3
EOF
```

设置主机名

根据解析，在每个节点设置合适的主机名

```bash
hostnamectl hostname ceph
```

准备基础先决条件

```bash
yum install podman chrony lvm2 systemd python3 bash-completion -y
reboot
```

为ceph软件准备仓库

```ini
yum install centos-release-ceph-reef.noarch epel-release.noarch -y
```

开启ntp同步

默认从公网同步，需要可以自己指定时间源

```bash
systemctl enable chronyd --now
```

安装cephadm

```bash
yum install cephadm ceph-common -y
```

# 引导新集群

引导新集群之前的所有操作都要在所有节点做好，此步骤仅在引导节点完成一次即可

**只有一个节点的时候，请加上--single-host-defaults参数**

```bash
cephadm bootstrap --mon-ip 192.168.8.21 --single-host-defaults --initial-dashboard-user admin --initial-dashboard-password LiXiaohui --dashboard-password-noupdate
```

多节点的时候，用下面的方式，不用加上面的参数


```bash
cephadm bootstrap --mon-ip 192.168.8.21 --initial-dashboard-user admin --initial-dashboard-password LiXiaohui --dashboard-password-noupdate
```
输出

这里要记住密码

```text
            User: admin
        Password: LiXiaohui
...
Bootstrap complete.
```

然后就可以用浏览器打开 https://192.168.8.21:8443 ceph的网页界面了

# 部署osd

查看现有硬盘

我还有nvme0n2、3、4空闲

```bash
[root@ceph ~]# lsblk
NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
sr0          11:0    1   9.6G  0 rom
nvme0n1     259:0    0   500G  0 disk
├─nvme0n1p1 259:1    0   600M  0 part /boot/efi
├─nvme0n1p2 259:2    0     1G  0 part /boot
└─nvme0n1p3 259:3    0 498.4G  0 part
  ├─cs-root 253:0    0 490.6G  0 lvm  /var/lib/containers/storage/overlay
  │                                   /
  └─cs-swap 253:1    0   7.9G  0 lvm  [SWAP]
nvme0n2     259:4    0   200G  0 disk
nvme0n3     259:5    0   200G  0 disk
nvme0n4     259:6    0   200G  0 disk
```
添加osd

```bash
[root@ceph ~]# ceph orch daemon add osd ceph:/dev/nvme0n2
Created osd(s) 0 on host 'ceph'
[root@ceph ~]# ceph orch daemon add osd ceph:/dev/nvme0n3
Created osd(s) 1 on host 'ceph'
[root@ceph ~]# ceph orch daemon add osd ceph:/dev/nvme0n4
Created osd(s) 2 on host 'ceph'
```

除了手工添加OSD之外，也可以开启自动添加osd功能

```bash
[root@ceph ~]# ceph orch apply osd --all-available-devices
```
输出

```text
Scheduled osd.all-available-devices update...
```

# 添加主机到集群

需要注意，新主机必须满足此文章的先决条件

需要将ssh密钥先分发给所有ceph节点

```bash
[root@ceph ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes

[root@ceph ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-2

Are you sure you want to continue connecting (yes/no/[fingerprint])? yes

[root@ceph ~]# ssh-copy-id -f -i /etc/ceph/ceph.pub root@ceph-3

Are you sure you want to continue connecting (yes/no/[fingerprint])? yes

```

更新集群主机列表

```bash
[root@ceph ~]# ceph orch host add ceph-2 192.168.8.22
Added host 'ceph-2' with addr '192.168.8.22'
[root@ceph ~]# ceph orch host add ceph-3 192.168.8.23
Added host 'ceph-3' with addr '192.168.8.23'

[root@ceph ~]# ceph orch host ls
HOST    ADDR          LABELS  STATUS
ceph    192.168.8.21  _admin
ceph-2  192.168.8.22
ceph-3  192.168.8.23
3 hosts in cluster

```

将机器添加到集群中需要一段时间才会生效，具体可以使用以下命令查看服务是否都正常

如果正常，全部都会处于running

```bash
ceph orch ps
```

# 部署对象存储

```bash
[root@ceph ~]# ceph orch apply rgw ceph
Scheduled rgw.ceph update...
[root@ceph ~]# ceph orch ps
NAME                    HOST    PORTS             STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID
...
rgw.ceph.ceph-2.opbroc  ceph-2  *:80              running (14s)     4s ago  14s    72.7M        -  18.2.2   c250cbc76d9d  3a924c58e2b0
rgw.ceph.ceph.zmhyvc    ceph    *:80              running (12s)     9s ago  12s    14.4M        -  18.2.2   c250cbc76d9d  63eb8bc25e37

```

# 部署文件存储

在ceph这台机器上启动了4个进程，并创建了一个名为Lxhfsname的文件系统，并同时启用了让系统自行决定几个进程合适的功能

```bash
[root@ceph ~]# ceph fs volume create lxhfsname --placement='4 ceph'
[root@ceph ~]# ceph mgr module enable mds_autoscaler
```

# 验证服务部署进程

```bash
[root@ceph ~]# ceph orch ps
NAME                       HOST    PORTS             STATUS         REFRESHED  AGE  MEM USE  MEM LIM  VERSION  IMAGE ID      CONTAINER ID
alertmanager.ceph          ceph    *:9093,9094       running (28m)   104s ago  67m    23.8M        -  0.25.0   c8568f914cd2  6817d2759005
ceph-exporter.ceph         ceph                      running (45m)   104s ago  68m    10.5M        -  18.2.2   c250cbc76d9d  e7b86ce3479e
ceph-exporter.ceph-2       ceph-2                    running (34m)     2m ago  34m    8762k        -  18.2.2   c250cbc76d9d  8fa4887ca83a
ceph-exporter.ceph-3       ceph-3                    running (14m)   105s ago  14m    7474k        -  18.2.2   c250cbc76d9d  7606f6438c5a
crash.ceph                 ceph                      running (45m)   104s ago  68m    6656k        -  18.2.2   c250cbc76d9d  1bd0a9f8d14d
crash.ceph-2               ceph-2                    running (34m)     2m ago  34m    6660k        -  18.2.2   c250cbc76d9d  2e2079c5bdce
crash.ceph-3               ceph-3                    running (14m)   105s ago  14m    6660k        -  18.2.2   c250cbc76d9d  7518716e82f2
grafana.ceph               ceph    *:3000            running (45m)   104s ago  66m    86.3M        -  9.4.7    954c08fa6188  80a23eaef5f8
mds.lxhfsname.ceph.iytdcn  ceph                      running (18m)   104s ago  18m    18.2M        -  18.2.2   c250cbc76d9d  09e65199db79
mds.lxhfsname.ceph.oesclk  ceph                      running (18m)   104s ago  18m    23.7M        -  18.2.2   c250cbc76d9d  1146bcb77c50
mds.lxhfsname.ceph.rxztca  ceph                      running (18m)   104s ago  18m    21.6M        -  18.2.2   c250cbc76d9d  55d4ee831ccf
mds.lxhfsname.ceph.sopdzs  ceph                      running (18m)   104s ago  18m    19.5M        -  18.2.2   c250cbc76d9d  825df5eaca7e
mgr.ceph-2.nkfkqy          ceph-2  *:8443,9283,8765  running (33m)     2m ago  33m     442M        -  18.2.2   c250cbc76d9d  e6817c824640
mgr.ceph.vrnjqk            ceph    *:9283,8765,8443  running (45m)   104s ago  69m     518M        -  18.2.2   c250cbc76d9d  26bcb313d1d5
mon.ceph                   ceph                      running (45m)   104s ago  69m     101M    2048M  18.2.2   c250cbc76d9d  6ea64cba669e
mon.ceph-2                 ceph-2                    running (33m)     2m ago  33m    63.5M    2048M  18.2.2   c250cbc76d9d  cab9f7eabbe8
mon.ceph-3                 ceph-3                    running (12m)   105s ago  12m    49.5M    2048M  18.2.2   c250cbc76d9d  8d76a28a7b0b
node-exporter.ceph         ceph    *:9100            running (45m)   104s ago  68m    14.8M        -  1.5.0    0da6a335fe13  b665630731f5
node-exporter.ceph-2       ceph-2  *:9100            running (34m)     2m ago  34m    16.7M        -  1.5.0    0da6a335fe13  2d93db20b917
node-exporter.ceph-3       ceph-3  *:9100            running (14m)   105s ago  14m    16.7M        -  1.5.0    0da6a335fe13  e087beaeb23b
osd.0                      ceph                      running (45m)   104s ago  54m    76.4M    4096M  18.2.2   c250cbc76d9d  768ebf1e254a
osd.1                      ceph                      running (45m)   104s ago  54m    90.7M    4096M  18.2.2   c250cbc76d9d  942a7c5ee26a
osd.2                      ceph                      running (45m)   104s ago  54m    73.4M    4096M  18.2.2   c250cbc76d9d  be445d63ba0c
osd.3                      ceph-2                    running (33m)     2m ago  33m    72.3M     950M  18.2.2   c250cbc76d9d  cbc67cd7e0b6
osd.4                      ceph-2                    running (33m)     2m ago  33m    84.1M     950M  18.2.2   c250cbc76d9d  fabe8ed4093d
osd.5                      ceph-2                    running (33m)     2m ago  33m    83.0M     950M  18.2.2   c250cbc76d9d  3d4d5eff6c72
osd.6                      ceph-3                    running (13m)   105s ago  13m    79.9M    2657M  18.2.2   c250cbc76d9d  6f3aacd4a797
osd.7                      ceph-3                    running (13m)   105s ago  13m    82.1M    2657M  18.2.2   c250cbc76d9d  74b8a9725c2e
osd.8                      ceph-3                    running (13m)   105s ago  12m    80.0M    2657M  18.2.2   c250cbc76d9d  1191f1f55625
prometheus.ceph            ceph    *:9095            running (12m)   104s ago  62m    74.7M        -  2.43.0   a07b618ecd1d  64dd3c2c654a
rgw.ceph.ceph-2.opbroc     ceph-2  *:80              running (23m)     2m ago  23m    83.0M        -  18.2.2   c250cbc76d9d  3a924c58e2b0
rgw.ceph.ceph.zmhyvc       ceph    *:80              running (23m)   104s ago  23m    82.7M        -  18.2.2   c250cbc76d9d  63eb8bc25e37
```

